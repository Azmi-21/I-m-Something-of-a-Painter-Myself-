{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monet Style Transfer - Optimized for Speed & Quality\n",
    "\n",
    "Uses **pre-trained VGG** for perceptual loss + lightweight generator.  \n",
    "~15-20 min training with good FID scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, shutil, tempfile, random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10-MIN Config\n",
    "DATA_DIR = '/kaggle/input/gan-getting-started'  \n",
    "OUTPUT_DIR = '/kaggle/working/outputs'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_SIZE = 256\n",
    "EPOCHS = 15           # Reduced\n",
    "BATCH_SIZE = 16       # Larger = faster\n",
    "LR_G = 2e-4\n",
    "LR_D = 4e-4\n",
    "\n",
    "LAMBDA_ADV = 1.0\n",
    "LAMBDA_PERC = 10.0\n",
    "\n",
    "print(f\"10-min config: {EPOCHS} epochs, batch={BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Photos and Monet Paintings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.photos = sorted(Path(data_dir, 'photo_jpg').glob('*.jpg'))\n",
    "        self.monets = sorted(Path(data_dir, 'monet_jpg').glob('*.jpg'))\n",
    "        self.tf = transforms.Compose([\n",
    "            transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "        ])\n",
    "        print(f\"Photos: {len(self.photos)}, Monet: {len(self.monets)}\")\n",
    "    def __len__(self): return len(self.photos)\n",
    "    def __getitem__(self, i):\n",
    "        p = Image.open(self.photos[i]).convert('RGB')\n",
    "        m = Image.open(self.monets[random.randint(0, len(self.monets)-1)]).convert('RGB')\n",
    "        return self.tf(p), self.tf(m)\n",
    "\n",
    "ds = Dataset(DATA_DIR)\n",
    "dl = DataLoader(ds, BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models + VGG Perceptual Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smaller ResBlock\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ch):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(ch, ch, 3), nn.InstanceNorm2d(ch), nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(1), nn.Conv2d(ch, ch, 3), nn.InstanceNorm2d(ch)\n",
    "        )\n",
    "    def forward(self, x): return x + self.block(x)\n",
    "\n",
    "# Smaller Generator - 4 ResBlocks instead of 6\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, nf=48):  # Reduced from 64\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3), nn.Conv2d(3, nf, 7), nn.InstanceNorm2d(nf), nn.ReLU(True),\n",
    "            nn.Conv2d(nf, nf*2, 3, 2, 1), nn.InstanceNorm2d(nf*2), nn.ReLU(True),\n",
    "            nn.Conv2d(nf*2, nf*4, 3, 2, 1), nn.InstanceNorm2d(nf*4), nn.ReLU(True),\n",
    "        )\n",
    "        self.res = nn.Sequential(*[ResBlock(nf*4) for _ in range(4)])  # 4 instead of 6\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nf*4, nf*2, 3, 2, 1, 1), nn.InstanceNorm2d(nf*2), nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(nf*2, nf, 3, 2, 1, 1), nn.InstanceNorm2d(nf), nn.ReLU(True),\n",
    "            nn.ReflectionPad2d(3), nn.Conv2d(nf, 3, 7), nn.Tanh()\n",
    "        )\n",
    "    def forward(self, x): return self.dec(self.res(self.enc(x)))\n",
    "\n",
    "# Smaller Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nf=48):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, nf, 4, 2, 1), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf, nf*2, 4, 2, 1), nn.InstanceNorm2d(nf*2), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf*2, nf*4, 4, 2, 1), nn.InstanceNorm2d(nf*4), nn.LeakyReLU(0.2, True),\n",
    "            nn.Conv2d(nf*4, 1, 4, 1, 1)\n",
    "        )\n",
    "    def forward(self, x): return self.model(x)\n",
    "\n",
    "# VGG Loss - use new API\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features[:16].eval()\n",
    "        for p in vgg.parameters(): p.requires_grad = False\n",
    "        self.vgg = vgg\n",
    "        self.register_buffer('mean', torch.tensor([0.485, 0.456, 0.406]).view(1,3,1,1))\n",
    "        self.register_buffer('std', torch.tensor([0.229, 0.224, 0.225]).view(1,3,1,1))\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        x = (x + 1) / 2; y = (y + 1) / 2\n",
    "        x = (x - self.mean) / self.std\n",
    "        y = (y - self.mean) / self.std\n",
    "        return F.l1_loss(self.vgg(x), self.vgg(y))\n",
    "\n",
    "G = Generator().to(device)\n",
    "D = Discriminator().to(device)\n",
    "vgg_loss = VGGLoss().to(device)\n",
    "\n",
    "print(f\"G: {sum(p.numel() for p in G.parameters()):,} | D: {sum(p.numel() for p in D.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers with TTUR\n",
    "optG = optim.Adam(G.parameters(), lr=LR_G, betas=(0.5, 0.999))\n",
    "optD = optim.Adam(D.parameters(), lr=LR_D, betas=(0.5, 0.999))\n",
    "mse = nn.MSELoss()\n",
    "G_losses, D_losses = [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Training with AMP\n",
    "start = time.time()\n",
    "fixed = next(iter(dl))[0][:4].to(device)\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    gL, dL = 0, 0\n",
    "    for photo, monet in tqdm(dl, desc=f\"E{epoch+1}\", leave=False):\n",
    "        photo, monet = photo.to(device), monet.to(device)\n",
    "        \n",
    "        # D\n",
    "        optD.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            fake = G(photo).detach()\n",
    "            loss_D = 0.5 * (mse(D(monet), torch.ones_like(D(monet))) + \n",
    "                           mse(D(fake), torch.zeros_like(D(fake))))\n",
    "        scaler.scale(loss_D).backward()\n",
    "        scaler.step(optD)\n",
    "        \n",
    "        # G\n",
    "        optG.zero_grad()\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            fake = G(photo)\n",
    "            loss_adv = mse(D(fake), torch.ones_like(D(fake)))\n",
    "            loss_perc = vgg_loss(fake, photo)\n",
    "            loss_G = LAMBDA_ADV * loss_adv + LAMBDA_PERC * loss_perc\n",
    "        scaler.scale(loss_G).backward()\n",
    "        scaler.step(optG)\n",
    "        scaler.update()\n",
    "        \n",
    "        gL += loss_G.item(); dL += loss_D.item()\n",
    "    \n",
    "    G_losses.append(gL/len(dl)); D_losses.append(dL/len(dl))\n",
    "    \n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(f\"E{epoch+1} D:{D_losses[-1]:.3f} G:{G_losses[-1]:.3f} [{(time.time()-start)/60:.1f}m]\")\n",
    "\n",
    "total = (time.time()-start)/60\n",
    "print(f\"\\n✓ Done in {total:.1f} min\")\n",
    "torch.save(G.state_dict(), f'{OUTPUT_DIR}/G.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. FID Evaluation\n",
    "\n",
    "**FID (Fréchet Inception Distance)** measures the quality of generated images.  \n",
    "Lower FID = better quality and more realistic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torch-fidelity for FID computation\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import torch_fidelity\n",
    "    print(\"✓ torch-fidelity already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing torch-fidelity...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch-fidelity'])\n",
    "    print(\"✓ torch-fidelity installed\")\n",
    "    \n",
    "# Now import torchmetrics FID\n",
    "try:\n",
    "    from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "    print(\"✓ FID evaluation module loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not load FID module: {e}\")\n",
    "    print(\"FID evaluation will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast FID (200 samples)\n",
    "print(\"Computing FID...\")\n",
    "temp = tempfile.mkdtemp()\n",
    "real_d, fake_d = f'{temp}/real', f'{temp}/fake'\n",
    "os.makedirs(real_d); os.makedirs(fake_d)\n",
    "\n",
    "try:\n",
    "    for i, p in enumerate(list(Path(DATA_DIR,'monet_jpg').glob('*.jpg'))[:200]):\n",
    "        shutil.copy(p, f'{real_d}/{i}.jpg')\n",
    "    \n",
    "    G.eval(); cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for photo, _ in dl:\n",
    "            if cnt >= 200: break\n",
    "            fake = G(photo.to(device))\n",
    "            for img in fake:\n",
    "                if cnt >= 200: break\n",
    "                transforms.ToPILImage()(((img.cpu()+1)/2).clamp(0,1)).save(f'{fake_d}/{cnt}.jpg')\n",
    "                cnt += 1\n",
    "    \n",
    "    import torch_fidelity\n",
    "    fid_score = torch_fidelity.calculate_metrics(input1=fake_d, input2=real_d, cuda=True, fid=True, verbose=False)['frechet_inception_distance']\n",
    "    print(f\"FID: {fid_score:.2f}\")\n",
    "except Exception as e:\n",
    "    print(f\"FID error: {e}\"); fid_score = None\n",
    "finally:\n",
    "    shutil.rmtree(temp, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visual Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "G.eval()\n",
    "fig, ax = plt.subplots(3, 6, figsize=(18, 9))\n",
    "batch = next(iter(DataLoader(ds, 6, shuffle=True)))\n",
    "photos = batch[0].to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fakes = G(photos).cpu()\n",
    "\n",
    "for i in range(6):\n",
    "    ax[0,i].imshow(((photos[i].cpu()+1)/2).permute(1,2,0).clamp(0,1))\n",
    "    ax[0,i].set_title('Photo'); ax[0,i].axis('off')\n",
    "    ax[1,i].imshow(((fakes[i]+1)/2).permute(1,2,0).clamp(0,1))\n",
    "    ax[1,i].set_title('Generated'); ax[1,i].axis('off')\n",
    "    real = transforms.Resize((256,256))(Image.open(ds.monets[i]))\n",
    "    ax[2,i].imshow(real); ax[2,i].set_title('Real Monet'); ax[2,i].axis('off')\n",
    "\n",
    "plt.suptitle(f'Photo → Monet (FID: {fid_score:.1f})' if fid_score else 'Photo → Monet', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/results.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More samples\n",
    "fig, ax = plt.subplots(4, 4, figsize=(12, 12))\n",
    "batch = next(iter(DataLoader(ds, 16, shuffle=True)))[0].to(device)\n",
    "with torch.no_grad(): out = G(batch).cpu()\n",
    "for i, a in enumerate(ax.flat):\n",
    "    a.imshow(((out[i]+1)/2).permute(1,2,0).clamp(0,1)); a.axis('off')\n",
    "plt.suptitle('Generated Monet-Style', fontsize=14)\n",
    "plt.tight_layout(); plt.savefig(f'{OUTPUT_DIR}/samples.png', dpi=150); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training: {total:.1f} min | FID: {fid_score:.1f}\" if fid_score else f\"Training: {total:.1f} min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss curves\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.plot(G_losses, label='G'); plt.plot(D_losses, label='D')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend(); plt.grid(alpha=0.3)\n",
    "plt.savefig(f'{OUTPUT_DIR}/loss.png'); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
