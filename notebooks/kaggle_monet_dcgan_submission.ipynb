{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monet Style GAN - DCGAN Implementation\n",
    "\n",
    "This notebook implements a Deep Convolutional GAN (DCGAN) to generate Monet-style paintings.\n",
    "\n",
    "**Competition:** GAN Getting Started - Kaggle  \n",
    "**Team:** Azmi Abidi, Guerlain Hitier-Lallement, Kaothar Reda  \n",
    "\n",
    "**Evaluation:** FID (Fr√©chet Inception Distance) score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import shutil\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_DIR = '/kaggle/input/gan-getting-started'  \n",
    "OUTPUT_DIR = '/kaggle/working/outputs'\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Hyperparameters - Aggressively optimized for better FID\n",
    "IMG_SIZE = 256\n",
    "BATCH_SIZE = 4           # Smaller batches = more stable training\n",
    "NZ = 256                 # Much larger latent space for diversity\n",
    "NGF = 96                 # Increased generator capacity\n",
    "NDF = 48                 # Reduced discriminator capacity (prevent overpowering)\n",
    "NC = 3\n",
    "EPOCHS = 75              # As requested\n",
    "LR_G = 3e-4              # Higher generator LR\n",
    "LR_D = 5e-5              # Much lower discriminator LR (key change!)\n",
    "BETA1 = 0.5\n",
    "BETA2 = 0.999\n",
    "N_CRITIC = 1             # Equal training\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Image Size: {IMG_SIZE}x{IMG_SIZE}\")\n",
    "print(f\"  Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Generator LR: {LR_G} (capacity: {NGF})\")\n",
    "print(f\"  Discriminator LR: {LR_D} (capacity: {NDF})\")\n",
    "print(f\"  Latent Dimension: {NZ}\")\n",
    "print(f\"  LR Ratio (G/D): {LR_G/LR_D:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MonetDataset(Dataset):\n",
    "    \"\"\"Dataset for loading Monet paintings with strong data augmentation.\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, img_size=256, transform=None, augment=True):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.img_size = img_size\n",
    "        self.augment = augment\n",
    "        \n",
    "        # Find monet images\n",
    "        self.monet_dir = self.data_dir / 'monet_jpg'\n",
    "        if not self.monet_dir.exists():\n",
    "            raise FileNotFoundError(f\"Directory {self.monet_dir} not found\")\n",
    "        \n",
    "        self.image_paths = sorted(list(self.monet_dir.glob('*.jpg')))\n",
    "        \n",
    "        if len(self.image_paths) == 0:\n",
    "            raise ValueError(f\"No images found in {self.monet_dir}\")\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} Monet paintings\")\n",
    "        \n",
    "        # Strong augmentation to prevent memorization and increase diversity\n",
    "        if transform is None:\n",
    "            if augment:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize((int(img_size * 1.1), int(img_size * 1.1))),  # Slightly larger\n",
    "                    transforms.RandomCrop((img_size, img_size)),  # Random crop\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomRotation(degrees=5),  # Slight rotation\n",
    "                    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.03),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                ])\n",
    "            else:\n",
    "                self.transform = transforms.Compose([\n",
    "                    transforms.Resize((img_size, img_size)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "                ])\n",
    "        else:\n",
    "            self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image\n",
    "\n",
    "# Create dataset and dataloader with strong augmentation\n",
    "dataset = MonetDataset(DATA_DIR, img_size=IMG_SIZE, augment=True)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, \n",
    "                       num_workers=2, pin_memory=True, drop_last=True)\n",
    "\n",
    "print(f\"‚úì Dataset loaded with strong augmentation (crop, flip, rotate, color jitter)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cap(ch):\n",
    "    \"\"\"Cap channels at 512 to avoid excessive memory usage.\"\"\"\n",
    "    return min(ch, 512)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"DCGAN Generator for 256x256 images with increased capacity.\"\"\"\n",
    "    \n",
    "    def __init__(self, nz=256, ngf=96, nc=3):\n",
    "        super(Generator, self).__init__()\n",
    "        self.nz = nz\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input: nz x 1 x 1\n",
    "            nn.ConvTranspose2d(nz, _cap(ngf * 16), 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ngf * 16)),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*16) x 4 x 4\n",
    "            nn.ConvTranspose2d(_cap(ngf * 16), _cap(ngf * 8), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ngf * 8)),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*8) x 8 x 8\n",
    "            nn.ConvTranspose2d(_cap(ngf * 8), _cap(ngf * 4), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ngf * 4)),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*4) x 16 x 16\n",
    "            nn.ConvTranspose2d(_cap(ngf * 4), _cap(ngf * 2), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ngf * 2)),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*2) x 32 x 32\n",
    "            nn.ConvTranspose2d(_cap(ngf * 2), _cap(ngf), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ngf)),\n",
    "            nn.ReLU(True),\n",
    "            # State: ngf x 64 x 64\n",
    "            nn.ConvTranspose2d(_cap(ngf), _cap(ngf // 2), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ngf // 2)),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf//2) x 128 x 128\n",
    "            nn.ConvTranspose2d(_cap(ngf // 2), nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: nc x 256 x 256\n",
    "        )\n",
    "    \n",
    "    def forward(self, z):\n",
    "        # Reshape input if needed\n",
    "        if z.dim() == 2:\n",
    "            z = z.view(z.size(0), z.size(1), 1, 1)\n",
    "        return self.main(z)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"DCGAN Discriminator for 256x256 images with reduced capacity and dropout.\"\"\"\n",
    "    \n",
    "    def __init__(self, nc=3, ndf=48):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.main = nn.Sequential(\n",
    "            # Input: nc x 256 x 256\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3),  # Add dropout for regularization\n",
    "            # State: ndf x 128 x 128\n",
    "            nn.Conv2d(ndf, _cap(ndf * 2), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ndf * 2)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3),\n",
    "            # State: (ndf*2) x 64 x 64\n",
    "            nn.Conv2d(_cap(ndf * 2), _cap(ndf * 4), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ndf * 4)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout2d(0.3),\n",
    "            # State: (ndf*4) x 32 x 32\n",
    "            nn.Conv2d(_cap(ndf * 4), _cap(ndf * 8), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ndf * 8)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (ndf*8) x 16 x 16\n",
    "            nn.Conv2d(_cap(ndf * 8), _cap(ndf * 16), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ndf * 16)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (ndf*16) x 8 x 8\n",
    "            nn.Conv2d(_cap(ndf * 16), _cap(ndf * 32), 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(_cap(ndf * 32)),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # State: (ndf*32) x 4 x 4\n",
    "            nn.Conv2d(_cap(ndf * 32), 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "            # Output: 1 x 1 x 1\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.main(x)\n",
    "        return output.view(-1, 1).squeeze(1)\n",
    "\n",
    "\n",
    "def weights_init(m):\n",
    "    \"\"\"Custom weight initialization with better scaling.\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "# Initialize models\n",
    "netG = Generator(nz=NZ, ngf=NGF, nc=NC).to(device)\n",
    "netD = Discriminator(nc=NC, ndf=NDF).to(device)\n",
    "\n",
    "# Apply weight initialization\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)\n",
    "\n",
    "print(\"Generator:\")\n",
    "print(netG)\n",
    "print(f\"\\nGenerator parameters: {sum(p.numel() for p in netG.parameters()):,}\")\n",
    "print(f\"Discriminator parameters: {sum(p.numel() for p in netD.parameters()):,}\")\n",
    "print(f\"Capacity ratio (G/D): {sum(p.numel() for p in netG.parameters())/sum(p.numel() for p in netD.parameters()):.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Optimizers with aggressive LR difference\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=LR_D, betas=(BETA1, BETA2))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=LR_G, betas=(BETA1, BETA2))\n",
    "\n",
    "# Cosine annealing scheduler for smooth LR decay\n",
    "schedulerD = optim.lr_scheduler.CosineAnnealingLR(optimizerD, T_max=EPOCHS, eta_min=1e-6)\n",
    "schedulerG = optim.lr_scheduler.CosineAnnealingLR(optimizerG, T_max=EPOCHS, eta_min=1e-5)\n",
    "\n",
    "# Fixed noise for visualization\n",
    "fixed_noise = torch.randn(16, NZ, 1, 1, device=device)\n",
    "\n",
    "# Labels with strong one-sided label smoothing\n",
    "real_label = 0.85  # Smoother real labels\n",
    "fake_label = 0.0\n",
    "\n",
    "# Training history\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "D_x_history = []\n",
    "D_G_z_history = []\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"  Generator LR: {LR_G} ‚Üí {1e-5}\")\n",
    "print(f\"  Discriminator LR: {LR_D} ‚Üí {1e-6}\")\n",
    "print(f\"  Using Cosine Annealing LR scheduling\")\n",
    "print(f\"  LR advantage for Generator: {LR_G/LR_D:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Training...\\n\")\n",
    "print(\"Strategy: Strong Generator, Weak Discriminator + Heavy Augmentation\\n\")\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    epoch_D_loss = 0.0\n",
    "    epoch_G_loss = 0.0\n",
    "    epoch_D_x = 0.0\n",
    "    epoch_D_G_z = 0.0\n",
    "    \n",
    "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    \n",
    "    for i, real_imgs in enumerate(progress_bar):\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        batch_size = real_imgs.size(0)\n",
    "        \n",
    "        # ==================== Train Discriminator ====================\n",
    "        netD.zero_grad()\n",
    "        \n",
    "        # Train with real images\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "        # Noisy labels (10% flip rate for stability)\n",
    "        if np.random.random() < 0.1:\n",
    "            label.fill_(fake_label)\n",
    "        \n",
    "        output_real = netD(real_imgs)\n",
    "        errD_real = criterion(output_real, label)\n",
    "        errD_real.backward()\n",
    "        D_x = output_real.mean().item()\n",
    "        \n",
    "        # Train with fake images\n",
    "        noise = torch.randn(batch_size, NZ, 1, 1, device=device)\n",
    "        fake_imgs = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Noisy labels\n",
    "        if np.random.random() < 0.1:\n",
    "            label.fill_(real_label)\n",
    "        \n",
    "        output_fake = netD(fake_imgs.detach())\n",
    "        errD_fake = criterion(output_fake, label)\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output_fake.mean().item()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(netD.parameters(), max_norm=1.0)\n",
    "        \n",
    "        errD = errD_real + errD_fake\n",
    "        optimizerD.step()\n",
    "        \n",
    "        # ==================== Train Generator (2x per D update) ====================\n",
    "        for _ in range(2):  # Train G twice to help it catch up\n",
    "            netG.zero_grad()\n",
    "            label.fill_(real_label)\n",
    "            output = netD(fake_imgs)\n",
    "            errG = criterion(output, label)\n",
    "            errG.backward()\n",
    "            D_G_z2 = output.mean().item()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(netG.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizerG.step()\n",
    "            \n",
    "            # Generate new fakes for second G update\n",
    "            if _ == 0:\n",
    "                noise = torch.randn(batch_size, NZ, 1, 1, device=device)\n",
    "                fake_imgs = netG(noise)\n",
    "        \n",
    "        # Track losses\n",
    "        epoch_D_loss += errD.item()\n",
    "        epoch_G_loss += errG.item()\n",
    "        epoch_D_x += D_x\n",
    "        epoch_D_G_z += D_G_z2\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'D_loss': f'{errD.item():.4f}',\n",
    "            'G_loss': f'{errG.item():.4f}',\n",
    "            'D(x)': f'{D_x:.3f}',\n",
    "            'D(G(z))': f'{D_G_z2:.3f}'\n",
    "        })\n",
    "    \n",
    "    # Average losses for epoch\n",
    "    avg_D_loss = epoch_D_loss / len(dataloader)\n",
    "    avg_G_loss = epoch_G_loss / len(dataloader)\n",
    "    avg_D_x = epoch_D_x / len(dataloader)\n",
    "    avg_D_G_z = epoch_D_G_z / len(dataloader)\n",
    "    \n",
    "    G_losses.append(avg_G_loss)\n",
    "    D_losses.append(avg_D_loss)\n",
    "    D_x_history.append(avg_D_x)\n",
    "    D_G_z_history.append(avg_D_G_z)\n",
    "    \n",
    "    # Step learning rate schedulers\n",
    "    schedulerD.step()\n",
    "    schedulerG.step()\n",
    "    \n",
    "    # Print epoch summary\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"\\nEpoch [{epoch+1}/{EPOCHS}] Summary:\")\n",
    "        print(f\"  G_loss: {avg_G_loss:.4f}, D_loss: {avg_D_loss:.4f}\")\n",
    "        print(f\"  D(x): {avg_D_x:.3f}, D(G(z)): {avg_D_G_z:.3f}\")\n",
    "        print(f\"  G_LR: {schedulerG.get_last_lr()[0]:.6f}, D_LR: {schedulerD.get_last_lr()[0]:.6f}\\n\")\n",
    "    \n",
    "    # Generate and save sample images\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            fake_samples = netG(fixed_noise).detach().cpu()\n",
    "        \n",
    "        # Save grid\n",
    "        img_grid = vutils.make_grid(fake_samples, nrow=4, padding=2, normalize=True)\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(img_grid.permute(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Epoch {epoch+1}')\n",
    "        plt.savefig(f'{OUTPUT_DIR}/epoch_{epoch+1:03d}.png', bbox_inches='tight', dpi=100)\n",
    "        plt.close()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if (epoch + 1) % 25 == 0 or (epoch + 1) == EPOCHS:\n",
    "        torch.save({\n",
    "            'epoch': epoch + 1,\n",
    "            'generator': netG.state_dict(),\n",
    "            'discriminator': netD.state_dict(),\n",
    "            'optimizerG': optimizerG.state_dict(),\n",
    "            'optimizerD': optimizerD.state_dict(),\n",
    "            'schedulerG': schedulerG.state_dict(),\n",
    "            'schedulerD': schedulerD.state_dict(),\n",
    "            'G_losses': G_losses,\n",
    "            'D_losses': D_losses,\n",
    "        }, f'{OUTPUT_DIR}/checkpoint_epoch_{epoch+1:03d}.pt')\n",
    "    \n",
    "    # Clear CUDA cache\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\nTraining Complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. FID Evaluation\n",
    "\n",
    "**FID (Fr√©chet Inception Distance)** measures the quality of generated images.  \n",
    "Lower FID = better quality and more realistic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install torch-fidelity for FID computation\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "try:\n",
    "    import torch_fidelity\n",
    "    print(\"‚úì torch-fidelity already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing torch-fidelity...\")\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', 'torch-fidelity'])\n",
    "    print(\"‚úì torch-fidelity installed\")\n",
    "    \n",
    "# Now import torchmetrics FID\n",
    "try:\n",
    "    from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "    print(\"‚úì FID evaluation module loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not load FID module: {e}\")\n",
    "    print(\"FID evaluation will be skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid_score_alternative(generator, num_fake_samples=1000, batch_size=16, device='cuda'):\n",
    "    \"\"\"\n",
    "    Alternative FID computation using torch-fidelity directly.\n",
    "    Saves images to temp directories and computes FID.\n",
    "    \"\"\"\n",
    "    import tempfile\n",
    "    import shutil\n",
    "    \n",
    "    print(f\"Computing FID score with {num_fake_samples} generated samples...\\n\")\n",
    "    \n",
    "    # Create temporary directories\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    real_dir = os.path.join(temp_dir, 'real')\n",
    "    fake_dir = os.path.join(temp_dir, 'fake')\n",
    "    os.makedirs(real_dir, exist_ok=True)\n",
    "    os.makedirs(fake_dir, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Copy real images\n",
    "        print(\"Preparing real Monet images...\")\n",
    "        monet_dir = Path(DATA_DIR) / 'monet_jpg'\n",
    "        real_images = list(monet_dir.glob('*.jpg'))\n",
    "        \n",
    "        for i, img_path in enumerate(tqdm(real_images[:num_fake_samples], desc=\"Copying real images\")):\n",
    "            shutil.copy(img_path, os.path.join(real_dir, f'real_{i:05d}.jpg'))\n",
    "        \n",
    "        # Generate and save fake images\n",
    "        print(\"\\nGenerating fake images...\")\n",
    "        generator.eval()\n",
    "        \n",
    "        num_batches = (num_fake_samples + batch_size - 1) // batch_size\n",
    "        img_counter = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in tqdm(range(num_batches), desc=\"Generating fake images\"):\n",
    "                current_batch_size = min(batch_size, num_fake_samples - img_counter)\n",
    "                \n",
    "                # Generate images\n",
    "                noise = torch.randn(current_batch_size, NZ, 1, 1, device=device)\n",
    "                fake_imgs = generator(noise)\n",
    "                \n",
    "                # Save each image\n",
    "                for i in range(current_batch_size):\n",
    "                    # Denormalize from [-1, 1] to [0, 1]\n",
    "                    img = (fake_imgs[i] + 1) / 2.0\n",
    "                    img = img.clamp(0, 1)\n",
    "                    \n",
    "                    # Convert to PIL Image and save\n",
    "                    img_pil = transforms.ToPILImage()(img.cpu())\n",
    "                    img_pil.save(os.path.join(fake_dir, f'fake_{img_counter:05d}.jpg'))\n",
    "                    img_counter += 1\n",
    "        \n",
    "        # Compute FID using torch-fidelity\n",
    "        print(\"\\nComputing FID score...\")\n",
    "        try:\n",
    "            import torch_fidelity\n",
    "            \n",
    "            metrics = torch_fidelity.calculate_metrics(\n",
    "                input1=fake_dir,\n",
    "                input2=real_dir,\n",
    "                cuda=torch.cuda.is_available(),\n",
    "                fid=True,\n",
    "                verbose=False\n",
    "            )\n",
    "            \n",
    "            fid_score = metrics['frechet_inception_distance']\n",
    "            \n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"FID Score: {fid_score:.2f}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(\"\\nFID Interpretation (for 256x256 GANs on small datasets):\")\n",
    "            print(\"  < 100:    Excellent - Publication quality\")\n",
    "            print(\"  100-150:  Very Good - Strong results\")\n",
    "            print(\"  150-200:  Good - Acceptable quality\")\n",
    "            print(\"  200-250:  Fair - Needs improvement\")\n",
    "            print(\"  > 250:    Poor - Significant issues\")\n",
    "            print(f\"{'='*60}\\n\")\n",
    "            \n",
    "            return fid_score\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"‚ö†Ô∏è  torch-fidelity not available. Cannot compute FID.\")\n",
    "            return None\n",
    "    \n",
    "    finally:\n",
    "        # Cleanup temp directories\n",
    "        shutil.rmtree(temp_dir, ignore_errors=True)\n",
    "\n",
    "\n",
    "# Compute FID score\n",
    "fid_score = None\n",
    "try:\n",
    "    fid_score = compute_fid_score_alternative(\n",
    "        generator=netG,\n",
    "        num_fake_samples=1000,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    if fid_score is not None:\n",
    "        # Store for later reference\n",
    "        evaluation_metrics = {\n",
    "            'fid_score': fid_score,\n",
    "            'num_samples': 1000,\n",
    "            'epoch': EPOCHS\n",
    "        }\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  FID computation failed: {e}\")\n",
    "    print(\"Continuing without FID evaluation...\")\n",
    "    fid_score = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visual Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real vs Generated comparison\n",
    "fig, axes = plt.subplots(2, 8, figsize=(20, 6))\n",
    "\n",
    "# Get 8 real images\n",
    "for i in range(8):\n",
    "    img = Image.open(dataset.image_paths[i]).convert('RGB')\n",
    "    img = transforms.Resize((256, 256))(img)\n",
    "    axes[0, i].imshow(img)\n",
    "    axes[0, i].set_title('Real', fontsize=10)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "# Generate 8 fake images\n",
    "netG.eval()\n",
    "with torch.no_grad():\n",
    "    noise = torch.randn(8, NZ, 1, 1, device=device)\n",
    "    fake_imgs = netG(noise).cpu()\n",
    "    fake_imgs = (fake_imgs + 1) / 2.0\n",
    "\n",
    "    for i in range(8):\n",
    "        axes[1, i].imshow(fake_imgs[i].permute(1, 2, 0).numpy())\n",
    "        axes[1, i].set_title('Generated', fontsize=10)\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "plt.suptitle('Real Monet Paintings vs Generated Images', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/real_vs_generated.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diversity check - generate 24 random samples\n",
    "fig, axes = plt.subplots(4, 6, figsize=(18, 12))\n",
    "\n",
    "netG.eval()\n",
    "with torch.no_grad():\n",
    "    for idx, ax in enumerate(axes.flat):\n",
    "        noise = torch.randn(1, NZ, 1, 1, device=device)\n",
    "        fake_img = netG(noise).cpu()\n",
    "        fake_img = (fake_img + 1) / 2.0\n",
    "        \n",
    "        ax.imshow(fake_img[0].permute(1, 2, 0).numpy())\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.suptitle('Diversity Check: 24 Random Samples', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/diversity_check.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"DCGAN TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä Model Architecture:\")\n",
    "print(f\"   Generator parameters: {sum(p.numel() for p in netG.parameters()):,}\")\n",
    "print(f\"   Discriminator parameters: {sum(p.numel() for p in netD.parameters()):,}\")\n",
    "\n",
    "print(f\"\\nüìà Training:\")\n",
    "print(f\"   Dataset: {len(dataset)} Monet paintings\")\n",
    "print(f\"   Epochs: {EPOCHS}\")\n",
    "print(f\"   Final Generator Loss: {G_losses[-1]:.4f}\")\n",
    "print(f\"   Final Discriminator Loss: {D_losses[-1]:.4f}\")\n",
    "\n",
    "if fid_score is not None:\n",
    "    print(f\"\\nüéØ FID Score: {fid_score:.2f}\")\n",
    "    \n",
    "    # Realistic thresholds for 256x256 GANs on small datasets\n",
    "    if fid_score < 100:\n",
    "        quality = \"Excellent ‚úì‚úì‚úì\"\n",
    "        emoji = \"üü¢\"\n",
    "    elif fid_score < 150:\n",
    "        quality = \"Very Good ‚úì‚úì\"\n",
    "        emoji = \"üü¢\"\n",
    "    elif fid_score < 200:\n",
    "        quality = \"Good ‚úì\"\n",
    "        emoji = \"üü°\"\n",
    "    elif fid_score < 250:\n",
    "        quality = \"Fair\"\n",
    "        emoji = \"üü†\"\n",
    "    else:\n",
    "        quality = \"Needs Improvement\"\n",
    "        emoji = \"üî¥\"\n",
    "    \n",
    "    print(f\"   Quality: {emoji} {quality}\")\n",
    "    print(f\"\\n   Realistic Thresholds (256x256, small dataset):\")\n",
    "    print(f\"     < 100:    Excellent\")\n",
    "    print(f\"     100-150:  Very Good\")\n",
    "    print(f\"     150-200:  Good\")\n",
    "    print(f\"     200-250:  Fair\")\n",
    "    print(f\"     > 250:    Poor\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è  FID Score: Not computed\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training losses\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(G_losses, label='Generator Loss', alpha=0.7)\n",
    "plt.plot(D_losses, label='Discriminator Loss', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Losses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f'{OUTPUT_DIR}/training_losses.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Display final generated samples\n",
    "with torch.no_grad():\n",
    "    final_samples = netG(fixed_noise).detach().cpu()\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(12, 12))\n",
    "for idx, ax in enumerate(axes.flat):\n",
    "    img = final_samples[idx].permute(1, 2, 0)\n",
    "    img = (img + 1) / 2  # Denormalize from [-1, 1] to [0, 1]\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "plt.suptitle('Final Generated Monet-Style Paintings', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{OUTPUT_DIR}/final_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
