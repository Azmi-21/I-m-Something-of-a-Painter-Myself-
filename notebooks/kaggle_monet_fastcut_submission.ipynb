{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¨ I'm Something of a Painter Myself - FastCUT Implementation\n",
    "\n",
    "This notebook implements a **lightweight FastCUT model** for the Kaggle competition to generate Monet-style paintings.\n",
    "\n",
    "**Key Features:**\n",
    "- Simplified FastCUT architecture (based on CUT paper, Park et al. ECCV 2020)\n",
    "- Optimized for <10 minute runtime on T4 GPU\n",
    "- 64x64 resolution for speed\n",
    "- FID evaluation using torchmetrics\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run this cell first, then restart kernel if needed)\n",
    "!pip install -q torchmetrics[image] torch-fidelity scipy\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Configuration =====\n",
    "class Config:\n",
    "    # Paths (adjust for Kaggle)\n",
    "    MONET_DIR = '/kaggle/input/gan-getting-started/monet_jpg'\n",
    "    PHOTO_DIR = '/kaggle/input/gan-getting-started/photo_jpg'\n",
    "    OUTPUT_DIR = '/kaggle/working'\n",
    "    FAKE_IMAGES_DIR = '/kaggle/working/fake_images'\n",
    "    REAL_IMAGES_DIR = '/kaggle/working/real_images'\n",
    "    \n",
    "    # Training parameters (optimized for speed)\n",
    "    IMAGE_SIZE = 64          # Small size for speed\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_ITERATIONS = 400     # ~8-9 minutes on T4\n",
    "    LR_G = 2e-4\n",
    "    LR_D = 2e-4\n",
    "    BETA1 = 0.5\n",
    "    BETA2 = 0.999\n",
    "    \n",
    "    # Model parameters (lightweight)\n",
    "    NGF = 48                 # Generator filters\n",
    "    NDF = 48                 # Discriminator filters\n",
    "    N_BLOCKS = 4             # ResNet blocks\n",
    "    NCE_LAYERS = [0, 2]      # Layers for NCE loss\n",
    "    NCE_TEMP = 0.07          # Temperature for NCE\n",
    "    LAMBDA_NCE = 1.0         # NCE loss weight\n",
    "    LAMBDA_GAN = 1.0         # GAN loss weight\n",
    "    \n",
    "    # Generation\n",
    "    NUM_FAKE_IMAGES = 250\n",
    "    NUM_REAL_IMAGES = 250\n",
    "    \n",
    "    # Misc\n",
    "    SEED = 42\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    LOG_INTERVAL = 50\n",
    "    SAVE_INTERVAL = 100\n",
    "\n",
    "cfg = Config()\n",
    "print(f\"Device: {cfg.DEVICE}\")\n",
    "if cfg.DEVICE == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.SEED)\n",
    "print(f\"Seed set to {cfg.SEED}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "os.makedirs(cfg.FAKE_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(cfg.REAL_IMAGES_DIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.OUTPUT_DIR, 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.OUTPUT_DIR, 'samples'), exist_ok=True)\n",
    "print(\"Directories created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    \"\"\"Dataset for loading images from a directory.\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, transform=None, max_samples=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get all image files\n",
    "        valid_ext = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "        self.image_paths = [\n",
    "            os.path.join(root_dir, f) for f in os.listdir(root_dir)\n",
    "            if os.path.splitext(f)[1].lower() in valid_ext\n",
    "        ]\n",
    "        \n",
    "        if max_samples:\n",
    "            self.image_paths = self.image_paths[:max_samples]\n",
    "            \n",
    "        print(f\"Loaded {len(self.image_paths)} images from {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize to [-1, 1]\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "monet_dataset = ImageDataset(cfg.MONET_DIR, transform=train_transform)\n",
    "photo_dataset = ImageDataset(cfg.PHOTO_DIR, transform=train_transform)\n",
    "\n",
    "# Create dataloaders\n",
    "monet_loader = DataLoader(\n",
    "    monet_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "photo_loader = DataLoader(\n",
    "    photo_dataset, \n",
    "    batch_size=cfg.BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "print(f\"Monet batches: {len(monet_loader)}, Photo batches: {len(photo_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample batch\n",
    "def show_batch(images, title=\"Sample Batch\", nrow=8):\n",
    "    \"\"\"Display a batch of images.\"\"\"\n",
    "    images = images * 0.5 + 0.5  # Denormalize\n",
    "    grid = make_grid(images, nrow=nrow, padding=2, normalize=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Show samples\n",
    "sample_monet = next(iter(monet_loader))\n",
    "sample_photo = next(iter(photo_loader))\n",
    "show_batch(sample_monet[:8], \"Real Monet Paintings\")\n",
    "show_batch(sample_photo[:8], \"Real Photos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. FastCUT Model Architecture\n",
    "\n",
    "Implementing a simplified version of:\n",
    "- **Generator**: ResNet-based encoder-decoder\n",
    "- **Discriminator**: PatchGAN discriminator\n",
    "- **PatchNCE Loss**: Contrastive loss for unpaired translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Building Blocks =====\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with instance normalization.\"\"\"\n",
    "    \n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Downsample(nn.Module):\n",
    "    \"\"\"Downsampling block.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, out_ch, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Upsample(nn.Module):\n",
    "    \"\"\"Upsampling block.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(out_ch),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Generator =====\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"ResNet-based Generator for FastCUT. Returns output and features for NCE.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch=3, out_ch=3, ngf=48, n_blocks=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial conv\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_ch, ngf, 7),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Downsampling\n",
    "        self.down1 = Downsample(ngf, ngf * 2)\n",
    "        self.down2 = Downsample(ngf * 2, ngf * 4)\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(ngf * 4) for _ in range(n_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Upsampling\n",
    "        self.up1 = Upsample(ngf * 4, ngf * 2)\n",
    "        self.up2 = Upsample(ngf * 2, ngf)\n",
    "        \n",
    "        # Final conv\n",
    "        self.final = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(ngf, out_ch, 7),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.nce_layers = [0, 2]\n",
    "    \n",
    "    def forward(self, x, return_features=False):\n",
    "        features = []\n",
    "        \n",
    "        x = self.initial(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        x = self.down1(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        x = self.down2(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        x = self.res_blocks(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        x = self.up1(x)\n",
    "        x = self.up2(x)\n",
    "        x = self.final(x)\n",
    "        \n",
    "        if return_features:\n",
    "            return x, [features[i] for i in self.nce_layers]\n",
    "        return x\n",
    "    \n",
    "    def encode(self, x):\n",
    "        \"\"\"Extract encoder features only.\"\"\"\n",
    "        features = []\n",
    "        \n",
    "        x = self.initial(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        x = self.down1(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        x = self.down2(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        x = self.res_blocks(x)\n",
    "        features.append(x)\n",
    "        \n",
    "        return [features[i] for i in self.nce_layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Discriminator =====\n",
    "\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    \"\"\"PatchGAN Discriminator.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_ch=3, ndf=48):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_ch, ndf, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf * 8, 1, 4, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== PatchNCE Loss =====\n",
    "\n",
    "class PatchSampleMLP(nn.Module):\n",
    "    \"\"\"MLP head for projecting features for NCE loss.\"\"\"\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(out_dim, out_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "\n",
    "\n",
    "class PatchNCELoss(nn.Module):\n",
    "    \"\"\"PatchNCE loss for contrastive learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, nce_temp=0.07, num_patches=64):\n",
    "        super().__init__()\n",
    "        self.nce_temp = nce_temp\n",
    "        self.num_patches = num_patches\n",
    "        self.cross_entropy = nn.CrossEntropyLoss(reduction='mean')\n",
    "    \n",
    "    def forward(self, feat_q, feat_k):\n",
    "        B, C, H, W = feat_q.shape\n",
    "        \n",
    "        feat_q = feat_q.view(B, C, -1)\n",
    "        feat_k = feat_k.view(B, C, -1)\n",
    "        \n",
    "        N = feat_q.shape[2]\n",
    "        num_patches = min(self.num_patches, N)\n",
    "        patch_ids = torch.randperm(N, device=feat_q.device)[:num_patches]\n",
    "        \n",
    "        feat_q = feat_q[:, :, patch_ids]\n",
    "        feat_k = feat_k[:, :, patch_ids]\n",
    "        \n",
    "        feat_q = F.normalize(feat_q, dim=1)\n",
    "        feat_k = F.normalize(feat_k, dim=1)\n",
    "        \n",
    "        total_loss = 0.0\n",
    "        for b in range(B):\n",
    "            q = feat_q[b].T\n",
    "            k = feat_k[b].T\n",
    "            \n",
    "            l_pos = torch.sum(q * k, dim=1, keepdim=True)\n",
    "            l_neg = torch.mm(q, k.T)\n",
    "            \n",
    "            logits = torch.cat([l_pos, l_neg], dim=1) / self.nce_temp\n",
    "            labels = torch.zeros(num_patches, dtype=torch.long, device=feat_q.device)\n",
    "            \n",
    "            total_loss += self.cross_entropy(logits, labels)\n",
    "        \n",
    "        return total_loss / B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Initialize Models =====\n",
    "\n",
    "def init_weights(m):\n",
    "    \"\"\"Initialize network weights.\"\"\"\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('BatchNorm') != -1 or classname.find('InstanceNorm') != -1:\n",
    "        if hasattr(m, 'weight') and m.weight is not None:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# Create models\n",
    "G = Generator(ngf=cfg.NGF, n_blocks=cfg.N_BLOCKS).to(cfg.DEVICE)\n",
    "D = PatchDiscriminator(ndf=cfg.NDF).to(cfg.DEVICE)\n",
    "\n",
    "G.apply(init_weights)\n",
    "D.apply(init_weights)\n",
    "\n",
    "# Create MLP heads for NCE loss\n",
    "with torch.no_grad():\n",
    "    dummy = torch.zeros(1, 3, cfg.IMAGE_SIZE, cfg.IMAGE_SIZE).to(cfg.DEVICE)\n",
    "    _, feats = G(dummy, return_features=True)\n",
    "    feat_dims = [f.shape[1] for f in feats]\n",
    "    print(f\"Feature dimensions for NCE: {feat_dims}\")\n",
    "\n",
    "mlp_heads = nn.ModuleList([\n",
    "    PatchSampleMLP(dim, 256).to(cfg.DEVICE) for dim in feat_dims\n",
    "])\n",
    "\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Generator params: {count_params(G):,}\")\n",
    "print(f\"Discriminator params: {count_params(D):,}\")\n",
    "print(f\"MLP heads params: {count_params(mlp_heads):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Loss Functions =====\n",
    "\n",
    "def gan_loss_lsgan(pred, target_is_real):\n",
    "    \"\"\"LSGAN loss.\"\"\"\n",
    "    target = torch.ones_like(pred) if target_is_real else torch.zeros_like(pred)\n",
    "    return F.mse_loss(pred, target)\n",
    "\n",
    "nce_loss_fn = PatchNCELoss(nce_temp=cfg.NCE_TEMP, num_patches=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Optimizers =====\n",
    "\n",
    "optimizer_G = optim.Adam(\n",
    "    list(G.parameters()) + list(mlp_heads.parameters()),\n",
    "    lr=cfg.LR_G, \n",
    "    betas=(cfg.BETA1, cfg.BETA2)\n",
    ")\n",
    "\n",
    "optimizer_D = optim.Adam(\n",
    "    D.parameters(), \n",
    "    lr=cfg.LR_D, \n",
    "    betas=(cfg.BETA1, cfg.BETA2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Training =====\n",
    "\n",
    "def train_fastcut():\n",
    "    \"\"\"Main training loop for FastCUT.\"\"\"\n",
    "    \n",
    "    G.train()\n",
    "    D.train()\n",
    "    mlp_heads.train()\n",
    "    \n",
    "    history = {'G_loss': [], 'D_loss': [], 'NCE_loss': [], 'GAN_loss': []}\n",
    "    \n",
    "    monet_iter = iter(monet_loader)\n",
    "    photo_iter = iter(photo_loader)\n",
    "    \n",
    "    fixed_photos = next(iter(photo_loader))[:8].to(cfg.DEVICE)\n",
    "    \n",
    "    print(f\"Starting training for {cfg.NUM_ITERATIONS} iterations...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    pbar = tqdm(range(1, cfg.NUM_ITERATIONS + 1), desc=\"Training\")\n",
    "    \n",
    "    for iteration in pbar:\n",
    "        # Get batch (with cycling)\n",
    "        try:\n",
    "            real_monet = next(monet_iter)\n",
    "        except StopIteration:\n",
    "            monet_iter = iter(monet_loader)\n",
    "            real_monet = next(monet_iter)\n",
    "        \n",
    "        try:\n",
    "            real_photo = next(photo_iter)\n",
    "        except StopIteration:\n",
    "            photo_iter = iter(photo_loader)\n",
    "            real_photo = next(photo_iter)\n",
    "        \n",
    "        real_monet = real_monet.to(cfg.DEVICE)\n",
    "        real_photo = real_photo.to(cfg.DEVICE)\n",
    "        \n",
    "        # ===== Train Generator =====\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        fake_monet, fake_feats = G(real_photo, return_features=True)\n",
    "        real_feats = G.encode(real_photo)\n",
    "        \n",
    "        # GAN loss\n",
    "        pred_fake = D(fake_monet)\n",
    "        loss_G_gan = gan_loss_lsgan(pred_fake, True)\n",
    "        \n",
    "        # NCE loss\n",
    "        loss_nce = 0.0\n",
    "        for i, (feat_q, feat_k, mlp) in enumerate(zip(fake_feats, real_feats, mlp_heads)):\n",
    "            B, C, H, W = feat_q.shape\n",
    "            feat_q_flat = feat_q.permute(0, 2, 3, 1).reshape(-1, C)\n",
    "            feat_k_flat = feat_k.permute(0, 2, 3, 1).reshape(-1, C)\n",
    "            \n",
    "            feat_q_proj = mlp(feat_q_flat).reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "            feat_k_proj = mlp(feat_k_flat).reshape(B, H, W, -1).permute(0, 3, 1, 2)\n",
    "            \n",
    "            loss_nce += nce_loss_fn(feat_q_proj, feat_k_proj)\n",
    "        \n",
    "        loss_nce = loss_nce / len(mlp_heads)\n",
    "        \n",
    "        loss_G = cfg.LAMBDA_GAN * loss_G_gan + cfg.LAMBDA_NCE * loss_nce\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # ===== Train Discriminator =====\n",
    "        optimizer_D.zero_grad()\n",
    "        \n",
    "        pred_real = D(real_monet)\n",
    "        loss_D_real = gan_loss_lsgan(pred_real, True)\n",
    "        \n",
    "        pred_fake = D(fake_monet.detach())\n",
    "        loss_D_fake = gan_loss_lsgan(pred_fake, False)\n",
    "        \n",
    "        loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        # Record\n",
    "        history['G_loss'].append(loss_G.item())\n",
    "        history['D_loss'].append(loss_D.item())\n",
    "        history['NCE_loss'].append(loss_nce.item())\n",
    "        history['GAN_loss'].append(loss_G_gan.item())\n",
    "        \n",
    "        pbar.set_postfix({'G': f\"{loss_G.item():.3f}\", 'D': f\"{loss_D.item():.3f}\", 'NCE': f\"{loss_nce.item():.3f}\"})\n",
    "        \n",
    "        if iteration % cfg.LOG_INTERVAL == 0:\n",
    "            elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "            print(f\"\\n[{iteration}/{cfg.NUM_ITERATIONS}] G: {loss_G.item():.4f}, D: {loss_D.item():.4f}, NCE: {loss_nce.item():.4f} | Time: {elapsed:.1f}min\")\n",
    "        \n",
    "        if iteration % cfg.SAVE_INTERVAL == 0:\n",
    "            G.eval()\n",
    "            with torch.no_grad():\n",
    "                fake_samples = G(fixed_photos)\n",
    "                fake_samples = fake_samples * 0.5 + 0.5\n",
    "                save_image(fake_samples, os.path.join(cfg.OUTPUT_DIR, 'samples', f'iter_{iteration:04d}.png'), nrow=4)\n",
    "            G.train()\n",
    "    \n",
    "    total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    print(f\"Training completed in {total_time:.2f} minutes\")\n",
    "    \n",
    "    torch.save({\n",
    "        'G': G.state_dict(),\n",
    "        'D': D.state_dict(),\n",
    "        'mlp_heads': mlp_heads.state_dict(),\n",
    "        'history': history\n",
    "    }, os.path.join(cfg.OUTPUT_DIR, 'checkpoints', 'fastcut_final.pth'))\n",
    "    print(\"Checkpoint saved.\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "history = train_fastcut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sample Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_images(generator, dataloader, num_images, output_dir):\n",
    "    \"\"\"Generate fake Monet images from photos.\"\"\"\n",
    "    generator.eval()\n",
    "    count = 0\n",
    "    photo_iter = iter(dataloader)\n",
    "    \n",
    "    print(f\"Generating {num_images} fake Monet images...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=num_images, desc=\"Generating\")\n",
    "        \n",
    "        while count < num_images:\n",
    "            try:\n",
    "                photos = next(photo_iter)\n",
    "            except StopIteration:\n",
    "                photo_iter = iter(dataloader)\n",
    "                photos = next(photo_iter)\n",
    "            \n",
    "            photos = photos.to(cfg.DEVICE)\n",
    "            fake_monets = generator(photos)\n",
    "            fake_monets = fake_monets * 0.5 + 0.5\n",
    "            \n",
    "            for img in fake_monets:\n",
    "                if count >= num_images:\n",
    "                    break\n",
    "                save_path = os.path.join(output_dir, f'fake_monet_{count:04d}.png')\n",
    "                save_image(img, save_path)\n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "    print(f\"Saved {count} fake Monet images to {output_dir}\")\n",
    "    return count\n",
    "\n",
    "num_generated = generate_fake_images(G, photo_loader, cfg.NUM_FAKE_IMAGES, cfg.FAKE_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_real_images(source_dir, output_dir, num_images, transform):\n",
    "    \"\"\"Copy and resize real Monet images for FID calculation.\"\"\"\n",
    "    valid_ext = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "    image_paths = [\n",
    "        os.path.join(source_dir, f) for f in os.listdir(source_dir)\n",
    "        if os.path.splitext(f)[1].lower() in valid_ext\n",
    "    ][:num_images]\n",
    "    \n",
    "    print(f\"Copying {len(image_paths)} real Monet images...\")\n",
    "    \n",
    "    for i, path in enumerate(tqdm(image_paths, desc=\"Copying\")):\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        img = img.resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), Image.BILINEAR)\n",
    "        save_path = os.path.join(output_dir, f'real_monet_{i:04d}.png')\n",
    "        img.save(save_path)\n",
    "    \n",
    "    print(f\"Saved {len(image_paths)} real Monet images to {output_dir}\")\n",
    "    return len(image_paths)\n",
    "\n",
    "num_real = copy_real_images(cfg.MONET_DIR, cfg.REAL_IMAGES_DIR, cfg.NUM_REAL_IMAGES, train_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. FID Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fid(real_dir, fake_dir, batch_size=32):\n",
    "    \"\"\"Compute FID score between real and fake images using torchmetrics.\"\"\"\n",
    "    print(\"Computing FID score...\")\n",
    "    \n",
    "    fid_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(cfg.DEVICE)\n",
    "    \n",
    "    real_dataset = ImageDataset(real_dir, transform=fid_transform)\n",
    "    real_loader = DataLoader(real_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Processing {len(real_dataset)} real images...\")\n",
    "    for batch in tqdm(real_loader, desc=\"Real images\"):\n",
    "        batch = batch.to(cfg.DEVICE)\n",
    "        fid.update(batch, real=True)\n",
    "    \n",
    "    fake_dataset = ImageDataset(fake_dir, transform=fid_transform)\n",
    "    fake_loader = DataLoader(fake_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "    \n",
    "    print(f\"Processing {len(fake_dataset)} fake images...\")\n",
    "    for batch in tqdm(fake_loader, desc=\"Fake images\"):\n",
    "        batch = batch.to(cfg.DEVICE)\n",
    "        fid.update(batch, real=False)\n",
    "    \n",
    "    fid_score = fid.compute().item()\n",
    "    \n",
    "    print(f\"\\n{'=' * 40}\")\n",
    "    print(f\"FID Score: {fid_score:.4f}\")\n",
    "    print(f\"{'=' * 40}\")\n",
    "    \n",
    "    return fid_score\n",
    "\n",
    "fid_score = compute_fid(cfg.REAL_IMAGES_DIR, cfg.FAKE_IMAGES_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_path = os.path.join(cfg.OUTPUT_DIR, 'fid_results.csv')\n",
    "\n",
    "with open(results_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['metric', 'value'])\n",
    "    writer.writerow(['fid_score', fid_score])\n",
    "    writer.writerow(['num_real_images', num_real])\n",
    "    writer.writerow(['num_fake_images', num_generated])\n",
    "    writer.writerow(['image_size', cfg.IMAGE_SIZE])\n",
    "    writer.writerow(['num_iterations', cfg.NUM_ITERATIONS])\n",
    "    writer.writerow(['model', 'FastCUT'])\n",
    "\n",
    "print(f\"Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Plot Training Curves =====\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].plot(history['G_loss'], label='G Loss', color='blue', alpha=0.7)\n",
    "axes[0, 0].set_title('Generator Loss')\n",
    "axes[0, 0].set_xlabel('Iteration')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history['D_loss'], label='D Loss', color='red', alpha=0.7)\n",
    "axes[0, 1].set_title('Discriminator Loss')\n",
    "axes[0, 1].set_xlabel('Iteration')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history['NCE_loss'], label='NCE Loss', color='green', alpha=0.7)\n",
    "axes[1, 0].set_title('PatchNCE Loss (Contrastive)')\n",
    "axes[1, 0].set_xlabel('Iteration')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history['GAN_loss'], label='GAN Loss', color='purple', alpha=0.7)\n",
    "axes[1, 1].set_title('GAN Loss')\n",
    "axes[1, 1].set_xlabel('Iteration')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(cfg.OUTPUT_DIR, 'training_curves.png'), dpi=150)\n",
    "plt.show()\n",
    "print(\"Training curves saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Display Generated Images Grid =====\n",
    "\n",
    "def display_generated_grid(fake_dir, num_images=16, nrow=4):\n",
    "    \"\"\"Display a grid of generated images.\"\"\"\n",
    "    image_files = sorted([f for f in os.listdir(fake_dir) if f.endswith('.png')])[:num_images]\n",
    "    \n",
    "    images = []\n",
    "    for fname in image_files:\n",
    "        img = Image.open(os.path.join(fake_dir, fname)).convert('RGB')\n",
    "        img_tensor = transforms.ToTensor()(img)\n",
    "        images.append(img_tensor)\n",
    "    \n",
    "    grid = make_grid(torch.stack(images), nrow=nrow, padding=2)\n",
    "    \n",
    "    plt.figure(figsize=(14, 14))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    plt.title(f'Generated Monet Images (FID: {fid_score:.2f})', fontsize=16)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(cfg.OUTPUT_DIR, 'generated_grid.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "display_generated_grid(cfg.FAKE_IMAGES_DIR, num_images=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Side-by-side Comparison: Photo -> Monet =====\n",
    "\n",
    "def show_translation_examples(generator, dataloader, num_examples=8):\n",
    "    \"\"\"Show photo to Monet translation examples.\"\"\"\n",
    "    generator.eval()\n",
    "    photos = next(iter(dataloader))[:num_examples].to(cfg.DEVICE)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake_monets = generator(photos)\n",
    "    \n",
    "    photos = photos * 0.5 + 0.5\n",
    "    fake_monets = fake_monets * 0.5 + 0.5\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_examples, figsize=(num_examples * 2, 4))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        axes[0, i].imshow(photos[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original Photo', fontsize=10)\n",
    "        \n",
    "        axes[1, i].imshow(fake_monets[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Generated Monet', fontsize=10)\n",
    "    \n",
    "    plt.suptitle('Photo -> Monet Style Transfer', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(cfg.OUTPUT_DIR, 'translation_examples.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "show_translation_examples(G, photo_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary & Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Print Final Summary =====\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"          FASTCUT TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Model Configuration:\")\n",
    "print(f\"  - Architecture: FastCUT (Lightweight)\")\n",
    "print(f\"  - Image Size: {cfg.IMAGE_SIZE}x{cfg.IMAGE_SIZE}\")\n",
    "print(f\"  - Generator Filters: {cfg.NGF}\")\n",
    "print(f\"  - Discriminator Filters: {cfg.NDF}\")\n",
    "print(f\"  - ResNet Blocks: {cfg.N_BLOCKS}\")\n",
    "print()\n",
    "print(\"Training:\")\n",
    "print(f\"  - Total Iterations: {cfg.NUM_ITERATIONS}\")\n",
    "print(f\"  - Batch Size: {cfg.BATCH_SIZE}\")\n",
    "print(f\"  - Learning Rate (G): {cfg.LR_G}\")\n",
    "print(f\"  - Learning Rate (D): {cfg.LR_D}\")\n",
    "print()\n",
    "print(\"Evaluation:\")\n",
    "print(f\"  - Real Images: {num_real}\")\n",
    "print(f\"  - Fake Images: {num_generated}\")\n",
    "print()\n",
    "print(\"+\" + \"-\" * 30 + \"+\")\n",
    "print(f\"|{'FID SCORE:':^15}{fid_score:^15.4f}|\")\n",
    "print(\"+\" + \"-\" * 30 + \"+\")\n",
    "print()\n",
    "print(\"Output Files:\")\n",
    "print(f\"  - Fake images: {cfg.FAKE_IMAGES_DIR}\")\n",
    "print(f\"  - Checkpoint: {os.path.join(cfg.OUTPUT_DIR, 'checkpoints', 'fastcut_final.pth')}\")\n",
    "print(f\"  - Results CSV: {results_path}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
