{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN Lite - Monet Style Transfer\n",
    "\n",
    "**Lightweight CycleGAN** optimized for ~30-45 min training on T4 GPU.\n",
    "\n",
    "Key optimizations:\n",
    "- 128x128 resolution (upscaled to 256x256 for submission)\n",
    "- 6 ResNet blocks (vs 9)\n",
    "- 32 base filters (vs 64)\n",
    "- 10 epochs\n",
    "- Batch size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchmetrics[image] torch-fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Optimized for speed\n",
    "class Config:\n",
    "    # Paths\n",
    "    MONET_DIR = '/kaggle/input/gan-getting-started/monet_jpg'\n",
    "    PHOTO_DIR = '/kaggle/input/gan-getting-started/photo_jpg'\n",
    "    OUTPUT_DIR = '/kaggle/working'\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = 128      # Train at 128, upscale to 256 for submission\n",
    "    OUTPUT_SIZE = 256     # Submission requirement\n",
    "    \n",
    "    # Training - Lighter settings\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 4        # Larger batch for speed\n",
    "    LR = 2e-4\n",
    "    BETA1 = 0.5\n",
    "    BETA2 = 0.999\n",
    "    \n",
    "    # Loss weights\n",
    "    LAMBDA_CYCLE = 10.0\n",
    "    LAMBDA_IDENTITY = 5.0\n",
    "    \n",
    "    # Architecture - Lighter\n",
    "    NGF = 32              # Reduced from 64\n",
    "    NDF = 32              # Reduced from 64\n",
    "    N_RESIDUAL = 6        # Reduced from 9\n",
    "    \n",
    "    # Generation\n",
    "    NUM_GENERATE = 7000\n",
    "    \n",
    "    # Misc\n",
    "    SEED = 42\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Set seeds\n",
    "random.seed(cfg.SEED)\n",
    "np.random.seed(cfg.SEED)\n",
    "torch.manual_seed(cfg.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.SEED)\n",
    "    torch.backends.cudnn.benchmark = True  # Speed optimization\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(os.path.join(cfg.OUTPUT_DIR, 'samples'), exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.OUTPUT_DIR, 'images'), exist_ok=True)\n",
    "\n",
    "print(f\"Training at {cfg.IMAGE_SIZE}x{cfg.IMAGE_SIZE}, output at {cfg.OUTPUT_SIZE}x{cfg.OUTPUT_SIZE}\")\n",
    "print(f\"Epochs: {cfg.EPOCHS}, Batch: {cfg.BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(root_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Loaded {len(self.images)} images from {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(os.path.join(self.root_dir, self.images[idx])).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "# Transforms\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(int(cfg.IMAGE_SIZE * 1.1), transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomCrop(cfg.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "# Dataloaders\n",
    "monet_dataset = ImageDataset(cfg.MONET_DIR, train_transform)\n",
    "photo_dataset = ImageDataset(cfg.PHOTO_DIR, train_transform)\n",
    "\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=cfg.NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "photo_loader = DataLoader(photo_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, \n",
    "                          num_workers=cfg.NUM_WORKERS, pin_memory=True, drop_last=True)\n",
    "\n",
    "print(f\"Monet batches: {len(monet_loader)}, Photo batches: {len(photo_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngf=32, n_res=6):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(3, ngf, 7),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Downsample 1\n",
    "            nn.Conv2d(ngf, ngf*2, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ngf*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Downsample 2\n",
    "            nn.Conv2d(ngf*2, ngf*4, 3, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ngf*4),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(*[ResBlock(ngf*4) for _ in range(n_res)])\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Upsample 1\n",
    "            nn.ConvTranspose2d(ngf*4, ngf*2, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(ngf*2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Upsample 2\n",
    "            nn.ConvTranspose2d(ngf*2, ngf, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True),\n",
    "            # Output\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(ngf, 3, 7),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.res_blocks(x)\n",
    "        return self.decoder(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf=32):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(3, ndf, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf, ndf*2, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf*2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf*2, ndf*4, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf*4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf*4, ndf*8, 4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(ndf*8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(ndf*8, 1, 4, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "def init_weights(m):\n",
    "    if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n",
    "        nn.init.normal_(m.weight, 0.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias, 0.0)\n",
    "    elif isinstance(m, nn.InstanceNorm2d) and m.weight is not None:\n",
    "        nn.init.normal_(m.weight, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "G_M = Generator(cfg.NGF, cfg.N_RESIDUAL).to(cfg.DEVICE)  # Photo -> Monet\n",
    "G_P = Generator(cfg.NGF, cfg.N_RESIDUAL).to(cfg.DEVICE)  # Monet -> Photo\n",
    "D_M = Discriminator(cfg.NDF).to(cfg.DEVICE)\n",
    "D_P = Discriminator(cfg.NDF).to(cfg.DEVICE)\n",
    "\n",
    "G_M.apply(init_weights)\n",
    "G_P.apply(init_weights)\n",
    "D_M.apply(init_weights)\n",
    "D_P.apply(init_weights)\n",
    "\n",
    "n_params = sum(p.numel() for p in G_M.parameters())\n",
    "print(f\"Generator params: {n_params:,}\")\n",
    "print(f\"Total params: {n_params * 2 + sum(p.numel() for p in D_M.parameters()) * 2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizers\n",
    "criterion_GAN = nn.MSELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "opt_G = optim.Adam(itertools.chain(G_M.parameters(), G_P.parameters()), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
    "opt_D = optim.Adam(itertools.chain(D_M.parameters(), D_P.parameters()), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
    "\n",
    "# LR scheduler\n",
    "scheduler_G = optim.lr_scheduler.LambdaLR(opt_G, lambda e: 1.0 - max(0, e - cfg.EPOCHS//2) / (cfg.EPOCHS//2 + 1))\n",
    "scheduler_D = optim.lr_scheduler.LambdaLR(opt_D, lambda e: 1.0 - max(0, e - cfg.EPOCHS//2) / (cfg.EPOCHS//2 + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "    \n",
    "    def push_and_pop(self, data):\n",
    "        result = []\n",
    "        for img in data:\n",
    "            img = img.unsqueeze(0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(img)\n",
    "                result.append(img)\n",
    "            elif random.random() > 0.5:\n",
    "                idx = random.randint(0, self.max_size - 1)\n",
    "                result.append(self.data[idx].clone())\n",
    "                self.data[idx] = img\n",
    "            else:\n",
    "                result.append(img)\n",
    "        return torch.cat(result, 0)\n",
    "\n",
    "buf_M = ReplayBuffer()\n",
    "buf_P = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    history = {'G': [], 'D': []}\n",
    "    fixed_photos = next(iter(photo_loader))[:4].to(cfg.DEVICE)\n",
    "    \n",
    "    print(f\"Training for {cfg.EPOCHS} epochs...\")\n",
    "    start = datetime.now()\n",
    "    \n",
    "    for epoch in range(cfg.EPOCHS):\n",
    "        G_M.train(); G_P.train(); D_M.train(); D_P.train()\n",
    "        \n",
    "        g_losses, d_losses = [], []\n",
    "        monet_iter = iter(monet_loader)\n",
    "        photo_iter = iter(photo_loader)\n",
    "        n_batches = min(len(monet_loader), len(photo_loader))\n",
    "        \n",
    "        pbar = tqdm(range(n_batches), desc=f\"Epoch {epoch+1}/{cfg.EPOCHS}\")\n",
    "        for _ in pbar:\n",
    "            try:\n",
    "                real_M = next(monet_iter).to(cfg.DEVICE)\n",
    "                real_P = next(photo_iter).to(cfg.DEVICE)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            \n",
    "            bs = real_M.size(0)\n",
    "            valid = torch.ones(bs, 1, 14, 14, device=cfg.DEVICE)\n",
    "            fake_label = torch.zeros(bs, 1, 14, 14, device=cfg.DEVICE)\n",
    "            \n",
    "            # ===== Train Generators =====\n",
    "            opt_G.zero_grad()\n",
    "            \n",
    "            # Identity\n",
    "            loss_id = (criterion_identity(G_M(real_M), real_M) + \n",
    "                       criterion_identity(G_P(real_P), real_P)) * cfg.LAMBDA_IDENTITY\n",
    "            \n",
    "            # GAN\n",
    "            fake_M = G_M(real_P)\n",
    "            fake_P = G_P(real_M)\n",
    "            loss_gan = (criterion_GAN(D_M(fake_M), valid) + \n",
    "                        criterion_GAN(D_P(fake_P), valid))\n",
    "            \n",
    "            # Cycle\n",
    "            loss_cycle = (criterion_cycle(G_P(fake_M), real_P) + \n",
    "                          criterion_cycle(G_M(fake_P), real_M)) * cfg.LAMBDA_CYCLE\n",
    "            \n",
    "            loss_G = loss_gan + loss_cycle + loss_id\n",
    "            loss_G.backward()\n",
    "            opt_G.step()\n",
    "            \n",
    "            # ===== Train Discriminators =====\n",
    "            opt_D.zero_grad()\n",
    "            \n",
    "            # D_M\n",
    "            fake_M_buf = buf_M.push_and_pop(fake_M.detach())\n",
    "            loss_D_M = (criterion_GAN(D_M(real_M), valid) + \n",
    "                        criterion_GAN(D_M(fake_M_buf), fake_label)) * 0.5\n",
    "            \n",
    "            # D_P\n",
    "            fake_P_buf = buf_P.push_and_pop(fake_P.detach())\n",
    "            loss_D_P = (criterion_GAN(D_P(real_P), valid) + \n",
    "                        criterion_GAN(D_P(fake_P_buf), fake_label)) * 0.5\n",
    "            \n",
    "            loss_D = loss_D_M + loss_D_P\n",
    "            loss_D.backward()\n",
    "            opt_D.step()\n",
    "            \n",
    "            g_losses.append(loss_G.item())\n",
    "            d_losses.append(loss_D.item())\n",
    "            pbar.set_postfix({'G': f\"{loss_G.item():.3f}\", 'D': f\"{loss_D.item():.3f}\"})\n",
    "        \n",
    "        # Record & schedule\n",
    "        history['G'].append(np.mean(g_losses))\n",
    "        history['D'].append(np.mean(d_losses))\n",
    "        scheduler_G.step()\n",
    "        scheduler_D.step()\n",
    "        \n",
    "        # Save samples\n",
    "        G_M.eval()\n",
    "        with torch.no_grad():\n",
    "            samples = G_M(fixed_photos) * 0.5 + 0.5\n",
    "            save_image(samples, os.path.join(cfg.OUTPUT_DIR, 'samples', f'epoch_{epoch+1:02d}.png'), nrow=2)\n",
    "        \n",
    "        elapsed = (datetime.now() - start).total_seconds() / 60\n",
    "        print(f\"Epoch {epoch+1} | G: {history['G'][-1]:.4f} | D: {history['D'][-1]:.4f} | Time: {elapsed:.1f}min\")\n",
    "    \n",
    "    print(f\"\\nTraining completed in {elapsed:.1f} minutes\")\n",
    "    return history\n",
    "\n",
    "history = train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission():\n",
    "    G_M.eval()\n",
    "    images_dir = os.path.join(cfg.OUTPUT_DIR, 'images')\n",
    "    \n",
    "    # Transform for generation (at training size, then upscale)\n",
    "    gen_transform = transforms.Compose([\n",
    "        transforms.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "    ])\n",
    "    \n",
    "    photo_files = [f for f in os.listdir(cfg.PHOTO_DIR) if f.endswith(('.jpg', '.png'))]\n",
    "    \n",
    "    print(f\"Generating {cfg.NUM_GENERATE} images...\")\n",
    "    count = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=cfg.NUM_GENERATE)\n",
    "        while count < cfg.NUM_GENERATE:\n",
    "            for f in photo_files:\n",
    "                if count >= cfg.NUM_GENERATE:\n",
    "                    break\n",
    "                \n",
    "                img = Image.open(os.path.join(cfg.PHOTO_DIR, f)).convert('RGB')\n",
    "                img_t = gen_transform(img).unsqueeze(0).to(cfg.DEVICE)\n",
    "                \n",
    "                fake = G_M(img_t)\n",
    "                fake = fake * 0.5 + 0.5\n",
    "                \n",
    "                # Upscale to 256x256 for submission\n",
    "                fake = F.interpolate(fake, size=(cfg.OUTPUT_SIZE, cfg.OUTPUT_SIZE), \n",
    "                                     mode='bicubic', align_corners=False)\n",
    "                fake = fake.clamp(0, 1)\n",
    "                \n",
    "                save_image(fake, os.path.join(images_dir, f'{count:05d}.jpg'))\n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "        pbar.close()\n",
    "    \n",
    "    # Create zip\n",
    "    zip_path = os.path.join(cfg.OUTPUT_DIR, 'images.zip')\n",
    "    print(\"Creating submission zip...\")\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zf:\n",
    "        for f in os.listdir(images_dir):\n",
    "            if f.endswith('.jpg'):\n",
    "                zf.write(os.path.join(images_dir, f), f)\n",
    "    \n",
    "    print(f\"Created: {zip_path} ({os.path.getsize(zip_path)/1024/1024:.1f} MB)\")\n",
    "    return count\n",
    "\n",
    "num_generated = generate_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FID Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "def compute_fid(real_dir, fake_dir, num_samples=300, batch_size=32):\n",
    "    \"\"\"Compute FID between real Monet images and generated images.\"\"\"\n",
    "    print(\"Computing FID score...\")\n",
    "    \n",
    "    # FID transform - Inception expects specific input\n",
    "    fid_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299), transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Initialize FID metric\n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(cfg.DEVICE)\n",
    "    \n",
    "    # Get real Monet images\n",
    "    real_files = [f for f in os.listdir(real_dir) if f.endswith(('.jpg', '.png'))][:num_samples]\n",
    "    print(f\"Processing {len(real_files)} real Monet images...\")\n",
    "    \n",
    "    real_images = []\n",
    "    for f in tqdm(real_files, desc=\"Loading real\"):\n",
    "        img = Image.open(os.path.join(real_dir, f)).convert('RGB')\n",
    "        img_t = fid_transform(img)\n",
    "        real_images.append(img_t)\n",
    "        \n",
    "        if len(real_images) == batch_size:\n",
    "            batch = torch.stack(real_images).to(cfg.DEVICE)\n",
    "            fid.update(batch, real=True)\n",
    "            real_images = []\n",
    "    \n",
    "    if real_images:\n",
    "        batch = torch.stack(real_images).to(cfg.DEVICE)\n",
    "        fid.update(batch, real=True)\n",
    "    \n",
    "    # Get fake/generated images\n",
    "    fake_files = [f for f in os.listdir(fake_dir) if f.endswith(('.jpg', '.png'))][:num_samples]\n",
    "    print(f\"Processing {len(fake_files)} generated images...\")\n",
    "    \n",
    "    fake_images = []\n",
    "    for f in tqdm(fake_files, desc=\"Loading fake\"):\n",
    "        img = Image.open(os.path.join(fake_dir, f)).convert('RGB')\n",
    "        img_t = fid_transform(img)\n",
    "        fake_images.append(img_t)\n",
    "        \n",
    "        if len(fake_images) == batch_size:\n",
    "            batch = torch.stack(fake_images).to(cfg.DEVICE)\n",
    "            fid.update(batch, real=False)\n",
    "            fake_images = []\n",
    "    \n",
    "    if fake_images:\n",
    "        batch = torch.stack(fake_images).to(cfg.DEVICE)\n",
    "        fid.update(batch, real=False)\n",
    "    \n",
    "    # Compute FID\n",
    "    fid_score = fid.compute().item()\n",
    "    \n",
    "    print(f\"\\n{'='*40}\")\n",
    "    print(f\"  FID Score: {fid_score:.2f}\")\n",
    "    print(f\"{'='*40}\")\n",
    "    \n",
    "    return fid_score\n",
    "\n",
    "# Compute FID\n",
    "images_dir = os.path.join(cfg.OUTPUT_DIR, 'images')\n",
    "fid_score = compute_fid(cfg.MONET_DIR, images_dir, num_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FID results to CSV\n",
    "import csv\n",
    "\n",
    "results_path = os.path.join(cfg.OUTPUT_DIR, 'fid_results.csv')\n",
    "with open(results_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['metric', 'value'])\n",
    "    writer.writerow(['fid_score', f'{fid_score:.4f}'])\n",
    "    writer.writerow(['num_epochs', cfg.EPOCHS])\n",
    "    writer.writerow(['image_size', cfg.IMAGE_SIZE])\n",
    "    writer.writerow(['output_size', cfg.OUTPUT_SIZE])\n",
    "    writer.writerow(['ngf', cfg.NGF])\n",
    "    writer.writerow(['n_residual', cfg.N_RESIDUAL])\n",
    "    writer.writerow(['num_generated', num_generated])\n",
    "\n",
    "print(f\"Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot losses\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
    "ax[0].plot(history['G'], label='Generator')\n",
    "ax[0].set_title('Generator Loss')\n",
    "ax[0].legend()\n",
    "ax[0].grid(True, alpha=0.3)\n",
    "\n",
    "ax[1].plot(history['D'], label='Discriminator', color='orange')\n",
    "ax[1].set_title('Discriminator Loss')\n",
    "ax[1].legend()\n",
    "ax[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(cfg.OUTPUT_DIR, 'losses.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show results\n",
    "G_M.eval()\n",
    "photos = next(iter(DataLoader(photo_dataset, batch_size=6, shuffle=True)))\n",
    "\n",
    "with torch.no_grad():\n",
    "    fakes = G_M(photos.to(cfg.DEVICE))\n",
    "\n",
    "photos = photos * 0.5 + 0.5\n",
    "fakes = fakes * 0.5 + 0.5\n",
    "\n",
    "fig, axes = plt.subplots(2, 6, figsize=(15, 5))\n",
    "for i in range(6):\n",
    "    axes[0, i].imshow(photos[i].permute(1,2,0).cpu())\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].imshow(fakes[i].permute(1,2,0).cpu())\n",
    "    axes[1, i].axis('off')\n",
    "axes[0, 0].set_title('Photo')\n",
    "axes[1, 0].set_title('Monet')\n",
    "plt.suptitle(f'Photo â†’ Monet (FID: {fid_score:.2f})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(cfg.OUTPUT_DIR, 'results.png'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"         CYCLEGAN LITE SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  - Training size: {cfg.IMAGE_SIZE}x{cfg.IMAGE_SIZE}\")\n",
    "print(f\"  - Output size: {cfg.OUTPUT_SIZE}x{cfg.OUTPUT_SIZE}\")\n",
    "print(f\"  - ResNet blocks: {cfg.N_RESIDUAL}\")\n",
    "print(f\"  - Base filters: {cfg.NGF}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Epochs: {cfg.EPOCHS}\")\n",
    "print(f\"  - Batch size: {cfg.BATCH_SIZE}\")\n",
    "print(f\"\\nResults:\")\n",
    "print(f\"  - Images generated: {num_generated}\")\n",
    "print(f\"\\n\" + \"+\"*50)\n",
    "print(f\"  FID SCORE: {fid_score:.2f}\")\n",
    "print(\"+\"*50)\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"  - Submission: {os.path.join(cfg.OUTPUT_DIR, 'images.zip')}\")\n",
    "print(f\"  - Results CSV: {results_path}\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
