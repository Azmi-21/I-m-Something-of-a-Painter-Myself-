{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monet CycleGAN - I'm Something of a Painter Myself\n",
    "\n",
    "This notebook implements a **CycleGAN** with:\n",
    "- **ResNet-9 Generator** (9 residual blocks)\n",
    "- **70x70 PatchGAN Discriminator**\n",
    "- **256x256 resolution** (competition requirement)\n",
    "- **Cycle consistency + Identity + Adversarial losses**\n",
    "\n",
    "Based on the original CycleGAN paper (Zhu et al., 2017) and proven Kaggle solutions achieving MiFID < 60."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torchmetrics[image] torch-fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "from datetime import datetime\n",
    "import csv\n",
    "import zipfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Paths\n",
    "    MONET_DIR = '/kaggle/input/gan-getting-started/monet_jpg'\n",
    "    PHOTO_DIR = '/kaggle/input/gan-getting-started/photo_jpg'\n",
    "    OUTPUT_DIR = '/kaggle/working'\n",
    "    \n",
    "    # Image settings\n",
    "    IMAGE_SIZE = 256  # Competition requirement\n",
    "    CHANNELS = 3\n",
    "    \n",
    "    # Training settings\n",
    "    EPOCHS = 25\n",
    "    BATCH_SIZE = 1  # CycleGAN typically uses batch_size=1\n",
    "    LR = 2e-4\n",
    "    BETA1 = 0.5\n",
    "    BETA2 = 0.999\n",
    "    \n",
    "    # Loss weights\n",
    "    LAMBDA_CYCLE = 10.0   # Cycle consistency weight\n",
    "    LAMBDA_IDENTITY = 5.0  # Identity loss weight\n",
    "    \n",
    "    # Architecture\n",
    "    NGF = 64  # Generator filters\n",
    "    NDF = 64  # Discriminator filters\n",
    "    N_RESIDUAL = 9  # ResNet blocks (9 for 256x256)\n",
    "    \n",
    "    # Generation\n",
    "    NUM_GENERATE = 7000  # Images to generate for submission\n",
    "    \n",
    "    # Misc\n",
    "    SEED = 42\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_WORKERS = 2\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Set seeds\n",
    "random.seed(cfg.SEED)\n",
    "np.random.seed(cfg.SEED)\n",
    "torch.manual_seed(cfg.SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(cfg.SEED)\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(os.path.join(cfg.OUTPUT_DIR, 'samples'), exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.OUTPUT_DIR, 'checkpoints'), exist_ok=True)\n",
    "os.makedirs(os.path.join(cfg.OUTPUT_DIR, 'images'), exist_ok=True)\n",
    "\n",
    "print(f\"Device: {cfg.DEVICE}\")\n",
    "print(f\"Image size: {cfg.IMAGE_SIZE}x{cfg.IMAGE_SIZE}\")\n",
    "print(f\"Epochs: {cfg.EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(root_dir) \n",
    "                       if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        print(f\"Loaded {len(self.images)} images from {root_dir}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, self.images[idx])\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms with augmentation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(int(cfg.IMAGE_SIZE * 1.12), transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.RandomCrop(cfg.IMAGE_SIZE),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((cfg.IMAGE_SIZE, cfg.IMAGE_SIZE), transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "monet_dataset = ImageDataset(cfg.MONET_DIR, transform=train_transform)\n",
    "photo_dataset = ImageDataset(cfg.PHOTO_DIR, transform=train_transform)\n",
    "\n",
    "monet_loader = DataLoader(monet_dataset, batch_size=cfg.BATCH_SIZE, \n",
    "                          shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)\n",
    "photo_loader = DataLoader(photo_dataset, batch_size=cfg.BATCH_SIZE, \n",
    "                          shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples\n",
    "def show_images(images, title, nrow=4):\n",
    "    images = images * 0.5 + 0.5  # Denormalize\n",
    "    grid = make_grid(images[:nrow], nrow=nrow, padding=2)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    plt.imshow(grid.permute(1, 2, 0).cpu().numpy())\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Show samples\n",
    "monet_batch = next(iter(DataLoader(monet_dataset, batch_size=4, shuffle=True)))\n",
    "photo_batch = next(iter(DataLoader(photo_dataset, batch_size=4, shuffle=True)))\n",
    "show_images(monet_batch, \"Monet Paintings\")\n",
    "show_images(photo_batch, \"Photos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CycleGAN Architecture\n",
    "\n",
    "### Generator: ResNet-9\n",
    "- Encoder: 2 downsampling layers\n",
    "- Transformer: 9 residual blocks\n",
    "- Decoder: 2 upsampling layers\n",
    "\n",
    "### Discriminator: 70x70 PatchGAN\n",
    "- Classifies 70x70 overlapping patches as real/fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with instance normalization.\"\"\"\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3),\n",
    "            nn.InstanceNorm2d(channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.ReflectionPad2d(1),\n",
    "            nn.Conv2d(channels, channels, 3),\n",
    "            nn.InstanceNorm2d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"\"\"ResNet-9 Generator for CycleGAN.\"\"\"\n",
    "    def __init__(self, in_channels=3, out_channels=3, ngf=64, n_residual=9):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Initial convolution\n",
    "        model = [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(in_channels, ngf, 7),\n",
    "            nn.InstanceNorm2d(ngf),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ]\n",
    "        \n",
    "        # Downsampling\n",
    "        in_features = ngf\n",
    "        out_features = ngf * 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv2d(in_features, out_features, 3, stride=2, padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features * 2\n",
    "        \n",
    "        # Residual blocks\n",
    "        for _ in range(n_residual):\n",
    "            model += [ResidualBlock(in_features)]\n",
    "        \n",
    "        # Upsampling\n",
    "        out_features = in_features // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose2d(in_features, out_features, 3, stride=2, padding=1, output_padding=1),\n",
    "                nn.InstanceNorm2d(out_features),\n",
    "                nn.ReLU(inplace=True)\n",
    "            ]\n",
    "            in_features = out_features\n",
    "            out_features = in_features // 2\n",
    "        \n",
    "        # Output layer\n",
    "        model += [\n",
    "            nn.ReflectionPad2d(3),\n",
    "            nn.Conv2d(ngf, out_channels, 7),\n",
    "            nn.Tanh()\n",
    "        ]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \"\"\"70x70 PatchGAN Discriminator.\"\"\"\n",
    "    def __init__(self, in_channels=3, ndf=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            # Layer 1: No normalization\n",
    "            nn.Conv2d(in_channels, ndf, 4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 2\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 3\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, stride=2, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Layer 4\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, stride=1, padding=1),\n",
    "            nn.InstanceNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Output layer\n",
    "            nn.Conv2d(ndf * 8, 1, 4, stride=1, padding=1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "def init_weights(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "        if hasattr(m, 'bias') and m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "    elif classname.find('InstanceNorm') != -1:\n",
    "        if m.weight is not None:\n",
    "            nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        if m.bias is not None:\n",
    "            nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "# Create generators and discriminators\n",
    "G_monet = Generator(ngf=cfg.NGF, n_residual=cfg.N_RESIDUAL).to(cfg.DEVICE)  # Photo -> Monet\n",
    "G_photo = Generator(ngf=cfg.NGF, n_residual=cfg.N_RESIDUAL).to(cfg.DEVICE)  # Monet -> Photo\n",
    "D_monet = Discriminator(ndf=cfg.NDF).to(cfg.DEVICE)  # Discriminate Monet\n",
    "D_photo = Discriminator(ndf=cfg.NDF).to(cfg.DEVICE)  # Discriminate Photo\n",
    "\n",
    "# Initialize weights\n",
    "G_monet.apply(init_weights)\n",
    "G_photo.apply(init_weights)\n",
    "D_monet.apply(init_weights)\n",
    "D_photo.apply(init_weights)\n",
    "\n",
    "# Count parameters\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Generator params: {count_params(G_monet):,}\")\n",
    "print(f\"Discriminator params: {count_params(D_monet):,}\")\n",
    "print(f\"Total params: {count_params(G_monet) * 2 + count_params(D_monet) * 2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions & Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = nn.MSELoss()  # LSGAN\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = optim.Adam(\n",
    "    itertools.chain(G_monet.parameters(), G_photo.parameters()),\n",
    "    lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2)\n",
    ")\n",
    "optimizer_D_monet = optim.Adam(D_monet.parameters(), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
    "optimizer_D_photo = optim.Adam(D_photo.parameters(), lr=cfg.LR, betas=(cfg.BETA1, cfg.BETA2))\n",
    "\n",
    "# Learning rate schedulers (linear decay after half epochs)\n",
    "def lambda_rule(epoch):\n",
    "    return 1.0 - max(0, epoch - cfg.EPOCHS // 2) / (cfg.EPOCHS // 2 + 1)\n",
    "\n",
    "scheduler_G = optim.lr_scheduler.LambdaLR(optimizer_G, lr_lambda=lambda_rule)\n",
    "scheduler_D_monet = optim.lr_scheduler.LambdaLR(optimizer_D_monet, lr_lambda=lambda_rule)\n",
    "scheduler_D_photo = optim.lr_scheduler.LambdaLR(optimizer_D_photo, lr_lambda=lambda_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"Buffer to store previously generated images for discriminator training.\"\"\"\n",
    "    def __init__(self, max_size=50):\n",
    "        self.max_size = max_size\n",
    "        self.data = []\n",
    "    \n",
    "    def push_and_pop(self, data):\n",
    "        result = []\n",
    "        for element in data:\n",
    "            element = element.unsqueeze(0)\n",
    "            if len(self.data) < self.max_size:\n",
    "                self.data.append(element)\n",
    "                result.append(element)\n",
    "            else:\n",
    "                if random.random() > 0.5:\n",
    "                    idx = random.randint(0, self.max_size - 1)\n",
    "                    result.append(self.data[idx].clone())\n",
    "                    self.data[idx] = element\n",
    "                else:\n",
    "                    result.append(element)\n",
    "        return torch.cat(result, 0)\n",
    "\n",
    "fake_monet_buffer = ReplayBuffer()\n",
    "fake_photo_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(epoch):\n",
    "    G_monet.train()\n",
    "    G_photo.train()\n",
    "    D_monet.train()\n",
    "    D_photo.train()\n",
    "    \n",
    "    losses = {'G': [], 'D_monet': [], 'D_photo': [], 'cycle': [], 'identity': []}\n",
    "    \n",
    "    monet_iter = iter(monet_loader)\n",
    "    photo_iter = iter(photo_loader)\n",
    "    \n",
    "    n_batches = min(len(monet_loader), len(photo_loader))\n",
    "    pbar = tqdm(range(n_batches), desc=f\"Epoch {epoch+1}/{cfg.EPOCHS}\")\n",
    "    \n",
    "    for i in pbar:\n",
    "        try:\n",
    "            real_monet = next(monet_iter).to(cfg.DEVICE)\n",
    "        except StopIteration:\n",
    "            monet_iter = iter(monet_loader)\n",
    "            real_monet = next(monet_iter).to(cfg.DEVICE)\n",
    "        \n",
    "        try:\n",
    "            real_photo = next(photo_iter).to(cfg.DEVICE)\n",
    "        except StopIteration:\n",
    "            photo_iter = iter(photo_loader)\n",
    "            real_photo = next(photo_iter).to(cfg.DEVICE)\n",
    "        \n",
    "        # Target tensors\n",
    "        valid = torch.ones((real_monet.size(0), 1, 30, 30), device=cfg.DEVICE)\n",
    "        fake = torch.zeros((real_monet.size(0), 1, 30, 30), device=cfg.DEVICE)\n",
    "        \n",
    "        # =================== Train Generators ===================\n",
    "        optimizer_G.zero_grad()\n",
    "        \n",
    "        # Identity loss\n",
    "        same_monet = G_monet(real_monet)\n",
    "        loss_identity_monet = criterion_identity(same_monet, real_monet) * cfg.LAMBDA_IDENTITY\n",
    "        \n",
    "        same_photo = G_photo(real_photo)\n",
    "        loss_identity_photo = criterion_identity(same_photo, real_photo) * cfg.LAMBDA_IDENTITY\n",
    "        \n",
    "        # GAN loss\n",
    "        fake_monet = G_monet(real_photo)\n",
    "        pred_fake_monet = D_monet(fake_monet)\n",
    "        loss_GAN_photo2monet = criterion_GAN(pred_fake_monet, valid)\n",
    "        \n",
    "        fake_photo = G_photo(real_monet)\n",
    "        pred_fake_photo = D_photo(fake_photo)\n",
    "        loss_GAN_monet2photo = criterion_GAN(pred_fake_photo, valid)\n",
    "        \n",
    "        # Cycle loss\n",
    "        recovered_photo = G_photo(fake_monet)\n",
    "        loss_cycle_photo = criterion_cycle(recovered_photo, real_photo) * cfg.LAMBDA_CYCLE\n",
    "        \n",
    "        recovered_monet = G_monet(fake_photo)\n",
    "        loss_cycle_monet = criterion_cycle(recovered_monet, real_monet) * cfg.LAMBDA_CYCLE\n",
    "        \n",
    "        # Total generator loss\n",
    "        loss_G = (loss_GAN_photo2monet + loss_GAN_monet2photo + \n",
    "                  loss_cycle_photo + loss_cycle_monet + \n",
    "                  loss_identity_monet + loss_identity_photo)\n",
    "        \n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # =================== Train Discriminator Monet ===================\n",
    "        optimizer_D_monet.zero_grad()\n",
    "        \n",
    "        pred_real = D_monet(real_monet)\n",
    "        loss_D_real = criterion_GAN(pred_real, valid)\n",
    "        \n",
    "        fake_monet_buf = fake_monet_buffer.push_and_pop(fake_monet.detach())\n",
    "        pred_fake = D_monet(fake_monet_buf)\n",
    "        loss_D_fake = criterion_GAN(pred_fake, fake)\n",
    "        \n",
    "        loss_D_monet = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_monet.backward()\n",
    "        optimizer_D_monet.step()\n",
    "        \n",
    "        # =================== Train Discriminator Photo ===================\n",
    "        optimizer_D_photo.zero_grad()\n",
    "        \n",
    "        pred_real = D_photo(real_photo)\n",
    "        loss_D_real = criterion_GAN(pred_real, valid)\n",
    "        \n",
    "        fake_photo_buf = fake_photo_buffer.push_and_pop(fake_photo.detach())\n",
    "        pred_fake = D_photo(fake_photo_buf)\n",
    "        loss_D_fake = criterion_GAN(pred_fake, fake)\n",
    "        \n",
    "        loss_D_photo = (loss_D_real + loss_D_fake) * 0.5\n",
    "        loss_D_photo.backward()\n",
    "        optimizer_D_photo.step()\n",
    "        \n",
    "        # Record losses\n",
    "        losses['G'].append(loss_G.item())\n",
    "        losses['D_monet'].append(loss_D_monet.item())\n",
    "        losses['D_photo'].append(loss_D_photo.item())\n",
    "        losses['cycle'].append((loss_cycle_photo + loss_cycle_monet).item())\n",
    "        losses['identity'].append((loss_identity_monet + loss_identity_photo).item())\n",
    "        \n",
    "        pbar.set_postfix({\n",
    "            'G': f\"{loss_G.item():.3f}\",\n",
    "            'D_m': f\"{loss_D_monet.item():.3f}\",\n",
    "            'D_p': f\"{loss_D_photo.item():.3f}\"\n",
    "        })\n",
    "    \n",
    "    return {k: np.mean(v) for k, v in losses.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_samples(epoch, photo_batch):\n",
    "    \"\"\"Save sample generated images.\"\"\"\n",
    "    G_monet.eval()\n",
    "    with torch.no_grad():\n",
    "        fake_monet = G_monet(photo_batch.to(cfg.DEVICE))\n",
    "    \n",
    "    # Denormalize\n",
    "    photo_batch = photo_batch * 0.5 + 0.5\n",
    "    fake_monet = fake_monet * 0.5 + 0.5\n",
    "    \n",
    "    # Create comparison grid\n",
    "    comparison = torch.cat([photo_batch[:4].cpu(), fake_monet[:4].cpu()], dim=0)\n",
    "    save_image(comparison, os.path.join(cfg.OUTPUT_DIR, 'samples', f'epoch_{epoch+1:03d}.png'), nrow=4)\n",
    "\n",
    "# Get fixed batch for visualization\n",
    "fixed_photos = next(iter(DataLoader(photo_dataset, batch_size=4, shuffle=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "print(f\"Starting training for {cfg.EPOCHS} epochs...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "history = {'G': [], 'D_monet': [], 'D_photo': [], 'cycle': [], 'identity': []}\n",
    "start_time = datetime.now()\n",
    "\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    epoch_losses = train_epoch(epoch)\n",
    "    \n",
    "    # Record history\n",
    "    for k, v in epoch_losses.items():\n",
    "        history[k].append(v)\n",
    "    \n",
    "    # Update learning rates\n",
    "    scheduler_G.step()\n",
    "    scheduler_D_monet.step()\n",
    "    scheduler_D_photo.step()\n",
    "    \n",
    "    # Save samples\n",
    "    save_samples(epoch, fixed_photos)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    elapsed = (datetime.now() - start_time).total_seconds() / 60\n",
    "    print(f\"Epoch {epoch+1}/{cfg.EPOCHS} | G: {epoch_losses['G']:.4f} | \"\n",
    "          f\"D_m: {epoch_losses['D_monet']:.4f} | D_p: {epoch_losses['D_photo']:.4f} | \"\n",
    "          f\"Time: {elapsed:.1f}min\")\n",
    "    \n",
    "    # Save checkpoint every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_monet': G_monet.state_dict(),\n",
    "            'G_photo': G_photo.state_dict(),\n",
    "            'D_monet': D_monet.state_dict(),\n",
    "            'D_photo': D_photo.state_dict(),\n",
    "            'history': history\n",
    "        }, os.path.join(cfg.OUTPUT_DIR, 'checkpoints', f'checkpoint_epoch_{epoch+1:03d}.pth'))\n",
    "\n",
    "total_time = (datetime.now() - start_time).total_seconds() / 60\n",
    "print(f\"\\nTraining completed in {total_time:.2f} minutes\")\n",
    "\n",
    "# Save final model\n",
    "torch.save(G_monet.state_dict(), os.path.join(cfg.OUTPUT_DIR, 'checkpoints', 'G_monet_final.pth'))\n",
    "print(\"Final model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Submission Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_submission_images(generator, photo_dir, output_dir, num_images=7000):\n",
    "    \"\"\"Generate images for Kaggle submission.\"\"\"\n",
    "    generator.eval()\n",
    "    \n",
    "    # Load all photos\n",
    "    photo_files = [f for f in os.listdir(photo_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    \n",
    "    print(f\"Generating {num_images} Monet-style images...\")\n",
    "    \n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(total=num_images)\n",
    "        \n",
    "        while count < num_images:\n",
    "            for photo_file in photo_files:\n",
    "                if count >= num_images:\n",
    "                    break\n",
    "                \n",
    "                # Load and transform photo\n",
    "                photo_path = os.path.join(photo_dir, photo_file)\n",
    "                photo = Image.open(photo_path).convert('RGB')\n",
    "                photo_tensor = test_transform(photo).unsqueeze(0).to(cfg.DEVICE)\n",
    "                \n",
    "                # Generate Monet-style image\n",
    "                fake_monet = generator(photo_tensor)\n",
    "                \n",
    "                # Denormalize and save\n",
    "                fake_monet = fake_monet * 0.5 + 0.5\n",
    "                save_path = os.path.join(output_dir, f'{count:05d}.jpg')\n",
    "                save_image(fake_monet, save_path)\n",
    "                \n",
    "                count += 1\n",
    "                pbar.update(1)\n",
    "        \n",
    "        pbar.close()\n",
    "    \n",
    "    print(f\"Generated {count} images to {output_dir}\")\n",
    "    return count\n",
    "\n",
    "# Generate submission images\n",
    "images_dir = os.path.join(cfg.OUTPUT_DIR, 'images')\n",
    "num_generated = generate_submission_images(G_monet, cfg.PHOTO_DIR, images_dir, cfg.NUM_GENERATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission zip\n",
    "zip_path = os.path.join(cfg.OUTPUT_DIR, 'images.zip')\n",
    "\n",
    "print(\"Creating submission zip...\")\n",
    "with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "    for filename in tqdm(os.listdir(images_dir)):\n",
    "        if filename.endswith('.jpg'):\n",
    "            filepath = os.path.join(images_dir, filename)\n",
    "            zipf.write(filepath, filename)\n",
    "\n",
    "print(f\"Submission zip created: {zip_path}\")\n",
    "print(f\"Zip size: {os.path.getsize(zip_path) / 1024 / 1024:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. FID Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "\n",
    "def compute_fid(real_dir, fake_dir, num_samples=300, batch_size=16):\n",
    "    \"\"\"Compute FID between real Monet images and generated images.\"\"\"\n",
    "    print(\"Computing FID score...\")\n",
    "    \n",
    "    # FID transform - Inception expects 299x299 images\n",
    "    fid_transform = transforms.Compose([\n",
    "        transforms.Resize((299, 299), transforms.InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    # Initialize FID metric\n",
    "    fid = FrechetInceptionDistance(feature=2048, normalize=True).to(cfg.DEVICE)\n",
    "    \n",
    "    # Process real Monet images\n",
    "    real_files = [f for f in os.listdir(real_dir) if f.lower().endswith(('.jpg', '.png'))][:num_samples]\n",
    "    print(f\"Processing {len(real_files)} real Monet images...\")\n",
    "    \n",
    "    real_batch = []\n",
    "    for f in tqdm(real_files, desc=\"Loading real\"):\n",
    "        img = Image.open(os.path.join(real_dir, f)).convert('RGB')\n",
    "        img_t = fid_transform(img)\n",
    "        real_batch.append(img_t)\n",
    "        \n",
    "        if len(real_batch) == batch_size:\n",
    "            batch = torch.stack(real_batch).to(cfg.DEVICE)\n",
    "            fid.update(batch, real=True)\n",
    "            real_batch = []\n",
    "    \n",
    "    if real_batch:\n",
    "        batch = torch.stack(real_batch).to(cfg.DEVICE)\n",
    "        fid.update(batch, real=True)\n",
    "    \n",
    "    # Process generated images\n",
    "    fake_files = [f for f in os.listdir(fake_dir) if f.lower().endswith(('.jpg', '.png'))][:num_samples]\n",
    "    print(f\"Processing {len(fake_files)} generated images...\")\n",
    "    \n",
    "    fake_batch = []\n",
    "    for f in tqdm(fake_files, desc=\"Loading fake\"):\n",
    "        img = Image.open(os.path.join(fake_dir, f)).convert('RGB')\n",
    "        img_t = fid_transform(img)\n",
    "        fake_batch.append(img_t)\n",
    "        \n",
    "        if len(fake_batch) == batch_size:\n",
    "            batch = torch.stack(fake_batch).to(cfg.DEVICE)\n",
    "            fid.update(batch, real=False)\n",
    "            fake_batch = []\n",
    "    \n",
    "    if fake_batch:\n",
    "        batch = torch.stack(fake_batch).to(cfg.DEVICE)\n",
    "        fid.update(batch, real=False)\n",
    "    \n",
    "    # Compute FID score\n",
    "    fid_score = fid.compute().item()\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"       FID Score: {fid_score:.2f}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    return fid_score\n",
    "\n",
    "# Compute FID\n",
    "fid_score = compute_fid(cfg.MONET_DIR, images_dir, num_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FID results to CSV\n",
    "results_path = os.path.join(cfg.OUTPUT_DIR, 'fid_results.csv')\n",
    "\n",
    "with open(results_path, 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['metric', 'value'])\n",
    "    writer.writerow(['fid_score', f'{fid_score:.4f}'])\n",
    "    writer.writerow(['num_epochs', cfg.EPOCHS])\n",
    "    writer.writerow(['image_size', cfg.IMAGE_SIZE])\n",
    "    writer.writerow(['ngf', cfg.NGF])\n",
    "    writer.writerow(['ndf', cfg.NDF])\n",
    "    writer.writerow(['n_residual', cfg.N_RESIDUAL])\n",
    "    writer.writerow(['lambda_cycle', cfg.LAMBDA_CYCLE])\n",
    "    writer.writerow(['lambda_identity', cfg.LAMBDA_IDENTITY])\n",
    "    writer.writerow(['num_generated', num_generated])\n",
    "    writer.writerow(['training_time_min', f'{total_time:.2f}'])\n",
    "\n",
    "print(f\"Results saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "axes[0, 0].plot(history['G'], label='Generator', color='blue')\n",
    "axes[0, 0].set_title('Generator Loss')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history['D_monet'], label='D_monet', color='red')\n",
    "axes[0, 1].plot(history['D_photo'], label='D_photo', color='orange')\n",
    "axes[0, 1].set_title('Discriminator Losses')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history['cycle'], label='Cycle Loss', color='green')\n",
    "axes[1, 0].set_title('Cycle Consistency Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history['identity'], label='Identity Loss', color='purple')\n",
    "axes[1, 1].set_title('Identity Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Training Curves (FID: {fid_score:.2f})', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(cfg.OUTPUT_DIR, 'training_curves.png'), dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show generated samples\n",
    "def show_translation_results(generator, num_samples=8):\n",
    "    generator.eval()\n",
    "    photos = next(iter(DataLoader(photo_dataset, batch_size=num_samples, shuffle=True)))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        fake_monets = generator(photos.to(cfg.DEVICE))\n",
    "    \n",
    "    photos = photos * 0.5 + 0.5\n",
    "    fake_monets = fake_monets * 0.5 + 0.5\n",
    "    \n",
    "    fig, axes = plt.subplots(2, num_samples, figsize=(num_samples * 2.5, 5))\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        axes[0, i].imshow(photos[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[0, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[0, i].set_title('Original Photo', fontsize=12)\n",
    "        \n",
    "        axes[1, i].imshow(fake_monets[i].permute(1, 2, 0).cpu().numpy())\n",
    "        axes[1, i].axis('off')\n",
    "        if i == 0:\n",
    "            axes[1, i].set_title('Generated Monet', fontsize=12)\n",
    "    \n",
    "    plt.suptitle(f'Photo to Monet Style Transfer (FID: {fid_score:.2f})', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(cfg.OUTPUT_DIR, 'translation_results.png'), dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "show_translation_results(G_monet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"           CYCLEGAN TRAINING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nArchitecture:\")\n",
    "print(f\"  - Generator: ResNet-{cfg.N_RESIDUAL} ({cfg.NGF} base filters)\")\n",
    "print(f\"  - Discriminator: 70x70 PatchGAN ({cfg.NDF} base filters)\")\n",
    "print(f\"  - Image Size: {cfg.IMAGE_SIZE}x{cfg.IMAGE_SIZE}\")\n",
    "print(f\"\\nTraining:\")\n",
    "print(f\"  - Epochs: {cfg.EPOCHS}\")\n",
    "print(f\"  - Batch Size: {cfg.BATCH_SIZE}\")\n",
    "print(f\"  - Learning Rate: {cfg.LR}\")\n",
    "print(f\"  - Lambda Cycle: {cfg.LAMBDA_CYCLE}\")\n",
    "print(f\"  - Lambda Identity: {cfg.LAMBDA_IDENTITY}\")\n",
    "print(f\"  - Training Time: {total_time:.2f} minutes\")\n",
    "print(f\"\\nGeneration:\")\n",
    "print(f\"  - Images Generated: {num_generated}\")\n",
    "print(f\"  - Submission File: {zip_path}\")\n",
    "print(f\"\\n\" + \"+\" * 60)\n",
    "print(f\"       FID SCORE: {fid_score:.2f}\")\n",
    "print(\"+\" * 60)\n",
    "print(f\"\\nResults saved to: {results_path}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nSubmit 'images.zip' to Kaggle for MiFID evaluation!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
